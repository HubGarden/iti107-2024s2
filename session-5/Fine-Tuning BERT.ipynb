{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107-2024S2/blob/main/session-5/Fine-Tuning%20BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning BERT for Text Classification\n",
        "\n",
        "One of the approaches where we can use BERT for downstream task such as text classification is to do fine-tuning of the pretrained model.\n",
        "\n",
        "In this lab, we will see how we can use a pretrained DistilBert Model and fine-tune it with custom training data for text classification task.\n",
        "\n",
        "At the end of this session, you will be able to:\n",
        "\n",
        "prepare data and use model-specific Tokenizer to format data suitable for use by the model\n",
        "configure the transformer model for fine-tuning\n",
        "train the model for binary and multi-class text classification"
      ],
      "metadata": {
        "id": "GEnB8NCyW21V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Transformers and other libraries\n",
        "\n",
        "If you are running this notebook in Google Colab, you will need to install the Hugging Face transformers library as it is not part of the standard environment.\n"
      ],
      "metadata": {
        "id": "j62zV1XXXUPD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "48spsQogssNh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets>=2.18.0 transformers>=4.38.2 sentence-transformers>=2.5.1 setfit>=1.0.3 accelerate>=0.27.2 seqeval>=1.2.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBeVnXxQWy7-"
      },
      "source": [
        "## Prepare Dataset\n",
        "\n",
        "The train set has 40000 samples. We will use only a small subset (e.g. 2500) samples for finetuning our pretrained model. Similarly we will use a smaller test set for evaluating our model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "# downloaded the datasets.\n",
        "test_data_url = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_test.csv'\n",
        "train_data_url = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_train.csv'\n",
        "\n",
        "train_data = load_dataset('csv', data_files=train_data_url, split=\"train\").shuffle(seed=128).select(range(2500))\n",
        "test_data = load_dataset('csv', data_files=test_data_url, split=\"train\").shuffle(seed=128).select(range(500))"
      ],
      "metadata": {
        "id": "ZWIl9VBV8_wv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-1UGAg7WAHk",
        "outputId": "f579b97b-b261-4bf2-d1c7-b8b2444daeef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:99: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load Model and Tokenizer\n",
        "model_id = \"bert-base-cased\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "\n",
        "Let us take a closer look at the output of the tokenization process.\n",
        "\n",
        "We notice that the tokenizer will return a dictionary of two items 'input_ids' and 'attention_mask'. The input_ids contains the IDs of the tokens. While the 'attention_mask' contains the masking pattern for those padded positions. If you are using BERT tokenizer, there will be additional item called 'token_type_ids'.\n",
        "\n",
        "We also notice that for the example sentence, the word 'Transformer' is being broken up into two tokens 'Trans' and '##former'. The '##' means that the rest of the token should be attached to the previous one.\n",
        "\n",
        "We also see that the tokenizer appended [CLS] to the beginning of the token sequence, and [SEP] at the end."
      ],
      "metadata": {
        "id": "TSR6FC9qbdSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"Transformer is really good for Natural Language Processing.\"\n",
        "\n",
        "encoding = tokenizer(test_sentence, padding=True, truncation=True)\n",
        "print(f\"Encoding keys:  {encoding.keys()}\\n\")\n",
        "\n",
        "print(f\"token ids: {encoding['input_ids']}\\n\")\n",
        "print(f\"attention_mask: {encoding['attention_mask']}\\n\")\n",
        "print(f\"tokens: {tokenizer.decode(encoding['input_ids'])}\")"
      ],
      "metadata": {
        "id": "LNRUxqVQbmRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r48vDo8fa33D"
      },
      "source": [
        "###Tokenization\n",
        "\n",
        "We will first convert the sentiment label from text to numeric label. We will need to create a data field called 'label' as the model will use this 'label' key as the target label.\n",
        "\n",
        "We will also use the model's (in this case the DistilBERT) tokenizer to produce the input data that are suitable to be used by the DistilBert model, e.g. the input_ids, the attention_mask.  It automatically append the [CLS] token in the front of the sequence of token_ids and the [SEP] token at the end of the sequence of token_ids , and also the attention mask for those padded positions in the input sequence of tokens.\n",
        "\n",
        "We also specify the DataCollator to use. Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset. To be able to build batches, data collators may apply some processing (like padding)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "f9443e06b1814fdd8004be5a7dcab9ec",
            "f45b1eb227234f4598a14fd5d39075a7",
            "7cb9b6a69a984264b6bf7a63930ae68f",
            "1f6bc5e3d50142bd9a784d8797263874",
            "fc5528b82a174aa4810718065c60f4d4",
            "80be597bdad24be9987fdfc69ceb6e70",
            "00d72f8f1a744da287fe1fecc4ada467",
            "7e493847df8444279ee2a619e906ee26",
            "d04c144e7659440ab7c3a65423c63fb0",
            "67e11a8b1b654b4ab589df09407bf144",
            "b5dd60026fb24dd38b55b2af541e3572",
            "240f76c17ddd4b55b5d428cc9c367041",
            "1cf9f6b247dd49488af3e1b31b46a9c6",
            "6e8e8218551c4e1abf372901414a9bf1",
            "5c6e3083b5624b31a8b6dfaf2bc3aaa5",
            "a6f42dff14d8461e8089f59cbd7ea2ee",
            "4db9776640e54cedbcb80bc46bc18c70",
            "b7205c659ef1463ba3c5f809917912a1",
            "ed26c166f97a49a49bb9c148c0caf261",
            "5fb29afb81244a668bd8c61770f36cee",
            "3a16735f95294188837d05af014e4393",
            "d43e5cbf5af54e2186ed00ec3c461dae"
          ]
        },
        "id": "5ySNm-a3WCFI",
        "outputId": "cb4e8b7d-9d02-46a2-820d-3240e4c1a9a1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9443e06b1814fdd8004be5a7dcab9ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "240f76c17ddd4b55b5d428cc9c367041"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "# Pad to the longest sequence in the batch\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "def convert_label(sample):\n",
        "    return {\n",
        "        \"label\" : 0 if sample['sentiment'] == 'negative' else 1\n",
        "    }\n",
        "    # sample['sentiment'] = 0 if sample['sentiment'] == 'negative' else 1\n",
        "    # return { \"label\": sample['sentiment']}\n",
        "    # return sample\n",
        "    # return {\n",
        "    #     \"text\":  sample['review'],\n",
        "    #     \"label\": sample['sentiment'] }\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"Tokenize input data\"\"\"\n",
        "    return tokenizer(examples[\"review\"], truncation=True)\n",
        "\n",
        "\n",
        "# Tokenize train/test data\n",
        "tokenized_train = train_data.map(convert_label).map(preprocess_function, remove_columns=['review', 'sentiment'], batched=True)\n",
        "tokenized_test = test_data.map(convert_label).map(preprocess_function, remove_columns=['review', 'sentiment'], batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_test)"
      ],
      "metadata": {
        "id": "P1CDrutbMx_P",
        "outputId": "b5499b3a-aa3c-4078-b363-bd0ef43788cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            "    num_rows: 500\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ1NM1gjbMD7"
      },
      "source": [
        "Define metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "Y724gUYyWIvq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Calculate F1 score\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    load_f1 = evaluate.load(\"f1\")\n",
        "    f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "    return {\"f1\": f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAm-sAl9bOPC"
      },
      "source": [
        "Train model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Dho6VcG9WK5u"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Training arguments for parameter tuning\n",
        "training_args = TrainingArguments(\n",
        "   \"model\",\n",
        "   learning_rate=2e-5,\n",
        "   per_device_train_batch_size=16,\n",
        "   per_device_eval_batch_size=16,\n",
        "   num_train_epochs=1,\n",
        "   weight_decay=0.01,\n",
        "   save_strategy=\"epoch\",\n",
        "   report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer which executes the training process\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_test,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "vOzl0WnSbVnY",
        "outputId": "163644c5-6c3d-42fb-d731-215b6b0331d3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [157/157 01:48, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=157, training_loss=0.2637446944121343, metrics={'train_runtime': 109.4419, 'train_samples_per_second': 22.843, 'train_steps_per_second': 1.435, 'total_flos': 650731195448640.0, 'train_loss': 0.2637446944121343, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkBUVlUYbUnn"
      },
      "source": [
        "Evaluate results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "wCI9uYDObWU8",
        "outputId": "c5269ccf-dc1d-49ab-a271-4ddbc6cf2e10"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32/32 00:14]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.31320637464523315,\n",
              " 'eval_f1': 0.884,\n",
              " 'eval_runtime': 16.445,\n",
              " 'eval_samples_per_second': 30.404,\n",
              " 'eval_steps_per_second': 1.946,\n",
              " 'epoch': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5gefwxOBllA"
      },
      "source": [
        "### Freeze Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0ZOuoe7Dj3c",
        "outputId": "88fdddb7-ec07-4be3-a06d-764b8111dfd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load Model and Tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI8vf_mnBniu",
        "outputId": "ff594996-8ac3-41fa-b597-92be30b43158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert.embeddings.word_embeddings.weight\n",
            "bert.embeddings.position_embeddings.weight\n",
            "bert.embeddings.token_type_embeddings.weight\n",
            "bert.embeddings.LayerNorm.weight\n",
            "bert.embeddings.LayerNorm.bias\n",
            "bert.encoder.layer.0.attention.self.query.weight\n",
            "bert.encoder.layer.0.attention.self.query.bias\n",
            "bert.encoder.layer.0.attention.self.key.weight\n",
            "bert.encoder.layer.0.attention.self.key.bias\n",
            "bert.encoder.layer.0.attention.self.value.weight\n",
            "bert.encoder.layer.0.attention.self.value.bias\n",
            "bert.encoder.layer.0.attention.output.dense.weight\n",
            "bert.encoder.layer.0.attention.output.dense.bias\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.0.intermediate.dense.weight\n",
            "bert.encoder.layer.0.intermediate.dense.bias\n",
            "bert.encoder.layer.0.output.dense.weight\n",
            "bert.encoder.layer.0.output.dense.bias\n",
            "bert.encoder.layer.0.output.LayerNorm.weight\n",
            "bert.encoder.layer.0.output.LayerNorm.bias\n",
            "bert.encoder.layer.1.attention.self.query.weight\n",
            "bert.encoder.layer.1.attention.self.query.bias\n",
            "bert.encoder.layer.1.attention.self.key.weight\n",
            "bert.encoder.layer.1.attention.self.key.bias\n",
            "bert.encoder.layer.1.attention.self.value.weight\n",
            "bert.encoder.layer.1.attention.self.value.bias\n",
            "bert.encoder.layer.1.attention.output.dense.weight\n",
            "bert.encoder.layer.1.attention.output.dense.bias\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.1.intermediate.dense.weight\n",
            "bert.encoder.layer.1.intermediate.dense.bias\n",
            "bert.encoder.layer.1.output.dense.weight\n",
            "bert.encoder.layer.1.output.dense.bias\n",
            "bert.encoder.layer.1.output.LayerNorm.weight\n",
            "bert.encoder.layer.1.output.LayerNorm.bias\n",
            "bert.encoder.layer.2.attention.self.query.weight\n",
            "bert.encoder.layer.2.attention.self.query.bias\n",
            "bert.encoder.layer.2.attention.self.key.weight\n",
            "bert.encoder.layer.2.attention.self.key.bias\n",
            "bert.encoder.layer.2.attention.self.value.weight\n",
            "bert.encoder.layer.2.attention.self.value.bias\n",
            "bert.encoder.layer.2.attention.output.dense.weight\n",
            "bert.encoder.layer.2.attention.output.dense.bias\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.2.intermediate.dense.weight\n",
            "bert.encoder.layer.2.intermediate.dense.bias\n",
            "bert.encoder.layer.2.output.dense.weight\n",
            "bert.encoder.layer.2.output.dense.bias\n",
            "bert.encoder.layer.2.output.LayerNorm.weight\n",
            "bert.encoder.layer.2.output.LayerNorm.bias\n",
            "bert.encoder.layer.3.attention.self.query.weight\n",
            "bert.encoder.layer.3.attention.self.query.bias\n",
            "bert.encoder.layer.3.attention.self.key.weight\n",
            "bert.encoder.layer.3.attention.self.key.bias\n",
            "bert.encoder.layer.3.attention.self.value.weight\n",
            "bert.encoder.layer.3.attention.self.value.bias\n",
            "bert.encoder.layer.3.attention.output.dense.weight\n",
            "bert.encoder.layer.3.attention.output.dense.bias\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.3.intermediate.dense.weight\n",
            "bert.encoder.layer.3.intermediate.dense.bias\n",
            "bert.encoder.layer.3.output.dense.weight\n",
            "bert.encoder.layer.3.output.dense.bias\n",
            "bert.encoder.layer.3.output.LayerNorm.weight\n",
            "bert.encoder.layer.3.output.LayerNorm.bias\n",
            "bert.encoder.layer.4.attention.self.query.weight\n",
            "bert.encoder.layer.4.attention.self.query.bias\n",
            "bert.encoder.layer.4.attention.self.key.weight\n",
            "bert.encoder.layer.4.attention.self.key.bias\n",
            "bert.encoder.layer.4.attention.self.value.weight\n",
            "bert.encoder.layer.4.attention.self.value.bias\n",
            "bert.encoder.layer.4.attention.output.dense.weight\n",
            "bert.encoder.layer.4.attention.output.dense.bias\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.4.intermediate.dense.weight\n",
            "bert.encoder.layer.4.intermediate.dense.bias\n",
            "bert.encoder.layer.4.output.dense.weight\n",
            "bert.encoder.layer.4.output.dense.bias\n",
            "bert.encoder.layer.4.output.LayerNorm.weight\n",
            "bert.encoder.layer.4.output.LayerNorm.bias\n",
            "bert.encoder.layer.5.attention.self.query.weight\n",
            "bert.encoder.layer.5.attention.self.query.bias\n",
            "bert.encoder.layer.5.attention.self.key.weight\n",
            "bert.encoder.layer.5.attention.self.key.bias\n",
            "bert.encoder.layer.5.attention.self.value.weight\n",
            "bert.encoder.layer.5.attention.self.value.bias\n",
            "bert.encoder.layer.5.attention.output.dense.weight\n",
            "bert.encoder.layer.5.attention.output.dense.bias\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.5.intermediate.dense.weight\n",
            "bert.encoder.layer.5.intermediate.dense.bias\n",
            "bert.encoder.layer.5.output.dense.weight\n",
            "bert.encoder.layer.5.output.dense.bias\n",
            "bert.encoder.layer.5.output.LayerNorm.weight\n",
            "bert.encoder.layer.5.output.LayerNorm.bias\n",
            "bert.encoder.layer.6.attention.self.query.weight\n",
            "bert.encoder.layer.6.attention.self.query.bias\n",
            "bert.encoder.layer.6.attention.self.key.weight\n",
            "bert.encoder.layer.6.attention.self.key.bias\n",
            "bert.encoder.layer.6.attention.self.value.weight\n",
            "bert.encoder.layer.6.attention.self.value.bias\n",
            "bert.encoder.layer.6.attention.output.dense.weight\n",
            "bert.encoder.layer.6.attention.output.dense.bias\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.6.intermediate.dense.weight\n",
            "bert.encoder.layer.6.intermediate.dense.bias\n",
            "bert.encoder.layer.6.output.dense.weight\n",
            "bert.encoder.layer.6.output.dense.bias\n",
            "bert.encoder.layer.6.output.LayerNorm.weight\n",
            "bert.encoder.layer.6.output.LayerNorm.bias\n",
            "bert.encoder.layer.7.attention.self.query.weight\n",
            "bert.encoder.layer.7.attention.self.query.bias\n",
            "bert.encoder.layer.7.attention.self.key.weight\n",
            "bert.encoder.layer.7.attention.self.key.bias\n",
            "bert.encoder.layer.7.attention.self.value.weight\n",
            "bert.encoder.layer.7.attention.self.value.bias\n",
            "bert.encoder.layer.7.attention.output.dense.weight\n",
            "bert.encoder.layer.7.attention.output.dense.bias\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.7.intermediate.dense.weight\n",
            "bert.encoder.layer.7.intermediate.dense.bias\n",
            "bert.encoder.layer.7.output.dense.weight\n",
            "bert.encoder.layer.7.output.dense.bias\n",
            "bert.encoder.layer.7.output.LayerNorm.weight\n",
            "bert.encoder.layer.7.output.LayerNorm.bias\n",
            "bert.encoder.layer.8.attention.self.query.weight\n",
            "bert.encoder.layer.8.attention.self.query.bias\n",
            "bert.encoder.layer.8.attention.self.key.weight\n",
            "bert.encoder.layer.8.attention.self.key.bias\n",
            "bert.encoder.layer.8.attention.self.value.weight\n",
            "bert.encoder.layer.8.attention.self.value.bias\n",
            "bert.encoder.layer.8.attention.output.dense.weight\n",
            "bert.encoder.layer.8.attention.output.dense.bias\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.8.intermediate.dense.weight\n",
            "bert.encoder.layer.8.intermediate.dense.bias\n",
            "bert.encoder.layer.8.output.dense.weight\n",
            "bert.encoder.layer.8.output.dense.bias\n",
            "bert.encoder.layer.8.output.LayerNorm.weight\n",
            "bert.encoder.layer.8.output.LayerNorm.bias\n",
            "bert.encoder.layer.9.attention.self.query.weight\n",
            "bert.encoder.layer.9.attention.self.query.bias\n",
            "bert.encoder.layer.9.attention.self.key.weight\n",
            "bert.encoder.layer.9.attention.self.key.bias\n",
            "bert.encoder.layer.9.attention.self.value.weight\n",
            "bert.encoder.layer.9.attention.self.value.bias\n",
            "bert.encoder.layer.9.attention.output.dense.weight\n",
            "bert.encoder.layer.9.attention.output.dense.bias\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.9.intermediate.dense.weight\n",
            "bert.encoder.layer.9.intermediate.dense.bias\n",
            "bert.encoder.layer.9.output.dense.weight\n",
            "bert.encoder.layer.9.output.dense.bias\n",
            "bert.encoder.layer.9.output.LayerNorm.weight\n",
            "bert.encoder.layer.9.output.LayerNorm.bias\n",
            "bert.encoder.layer.10.attention.self.query.weight\n",
            "bert.encoder.layer.10.attention.self.query.bias\n",
            "bert.encoder.layer.10.attention.self.key.weight\n",
            "bert.encoder.layer.10.attention.self.key.bias\n",
            "bert.encoder.layer.10.attention.self.value.weight\n",
            "bert.encoder.layer.10.attention.self.value.bias\n",
            "bert.encoder.layer.10.attention.output.dense.weight\n",
            "bert.encoder.layer.10.attention.output.dense.bias\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.10.intermediate.dense.weight\n",
            "bert.encoder.layer.10.intermediate.dense.bias\n",
            "bert.encoder.layer.10.output.dense.weight\n",
            "bert.encoder.layer.10.output.dense.bias\n",
            "bert.encoder.layer.10.output.LayerNorm.weight\n",
            "bert.encoder.layer.10.output.LayerNorm.bias\n",
            "bert.encoder.layer.11.attention.self.query.weight\n",
            "bert.encoder.layer.11.attention.self.query.bias\n",
            "bert.encoder.layer.11.attention.self.key.weight\n",
            "bert.encoder.layer.11.attention.self.key.bias\n",
            "bert.encoder.layer.11.attention.self.value.weight\n",
            "bert.encoder.layer.11.attention.self.value.bias\n",
            "bert.encoder.layer.11.attention.output.dense.weight\n",
            "bert.encoder.layer.11.attention.output.dense.bias\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.11.intermediate.dense.weight\n",
            "bert.encoder.layer.11.intermediate.dense.bias\n",
            "bert.encoder.layer.11.output.dense.weight\n",
            "bert.encoder.layer.11.output.dense.bias\n",
            "bert.encoder.layer.11.output.LayerNorm.weight\n",
            "bert.encoder.layer.11.output.LayerNorm.bias\n",
            "bert.pooler.dense.weight\n",
            "bert.pooler.dense.bias\n",
            "classifier.weight\n",
            "classifier.bias\n"
          ]
        }
      ],
      "source": [
        "# Print layer names\n",
        "for name, param in model.named_parameters():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "FnpGOry_Bm36"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "\n",
        "     # Trainable classification head\n",
        "     if name.startswith(\"classifier\"):\n",
        "        param.requires_grad = True\n",
        "\n",
        "      # Freeze everything else\n",
        "     else:\n",
        "        param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf_wYzpMB4uX",
        "outputId": "989ae40a-07e7-410e-81c1-4b4a05a9fdc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter: bert.embeddings.word_embeddings.weight ----- False\n",
            "Parameter: bert.embeddings.position_embeddings.weight ----- False\n",
            "Parameter: bert.embeddings.token_type_embeddings.weight ----- False\n",
            "Parameter: bert.embeddings.LayerNorm.weight ----- False\n",
            "Parameter: bert.embeddings.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.0.attention.self.query.weight ----- False\n",
            "Parameter: bert.encoder.layer.0.attention.self.query.bias ----- False\n",
            "Parameter: bert.encoder.layer.0.attention.self.key.weight ----- False\n",
            "Parameter: bert.encoder.layer.0.attention.self.key.bias ----- False\n",
            "Parameter: bert.encoder.layer.0.attention.self.value.weight ----- False\n",
            "Parameter: bert.encoder.layer.0.attention.self.value.bias ----- False\n",
            "Parameter: bert.encoder.layer.0.attention.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.0.attention.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.0.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.0.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.0.intermediate.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.0.intermediate.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.0.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.0.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.0.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.0.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.1.attention.self.query.weight ----- False\n",
            "Parameter: bert.encoder.layer.1.attention.self.query.bias ----- False\n",
            "Parameter: bert.encoder.layer.1.attention.self.key.weight ----- False\n",
            "Parameter: bert.encoder.layer.1.attention.self.key.bias ----- False\n",
            "Parameter: bert.encoder.layer.1.attention.self.value.weight ----- False\n",
            "Parameter: bert.encoder.layer.1.attention.self.value.bias ----- False\n",
            "Parameter: bert.encoder.layer.1.attention.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.1.attention.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.1.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.1.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.1.intermediate.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.1.intermediate.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.1.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.1.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.1.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.1.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.2.attention.self.query.weight ----- False\n",
            "Parameter: bert.encoder.layer.2.attention.self.query.bias ----- False\n",
            "Parameter: bert.encoder.layer.2.attention.self.key.weight ----- False\n",
            "Parameter: bert.encoder.layer.2.attention.self.key.bias ----- False\n",
            "Parameter: bert.encoder.layer.2.attention.self.value.weight ----- False\n",
            "Parameter: bert.encoder.layer.2.attention.self.value.bias ----- False\n",
            "Parameter: bert.encoder.layer.2.attention.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.2.attention.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.2.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.2.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.2.intermediate.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.2.intermediate.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.2.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.2.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.2.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.2.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.3.attention.self.query.weight ----- False\n",
            "Parameter: bert.encoder.layer.3.attention.self.query.bias ----- False\n",
            "Parameter: bert.encoder.layer.3.attention.self.key.weight ----- False\n",
            "Parameter: bert.encoder.layer.3.attention.self.key.bias ----- False\n",
            "Parameter: bert.encoder.layer.3.attention.self.value.weight ----- False\n",
            "Parameter: bert.encoder.layer.3.attention.self.value.bias ----- False\n",
            "Parameter: bert.encoder.layer.3.attention.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.3.attention.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.3.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.3.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.3.intermediate.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.3.intermediate.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.3.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.3.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.3.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.3.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.4.attention.self.query.weight ----- False\n",
            "Parameter: bert.encoder.layer.4.attention.self.query.bias ----- False\n",
            "Parameter: bert.encoder.layer.4.attention.self.key.weight ----- False\n",
            "Parameter: bert.encoder.layer.4.attention.self.key.bias ----- False\n",
            "Parameter: bert.encoder.layer.4.attention.self.value.weight ----- False\n",
            "Parameter: bert.encoder.layer.4.attention.self.value.bias ----- False\n",
            "Parameter: bert.encoder.layer.4.attention.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.4.attention.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.4.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.4.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.4.intermediate.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.4.intermediate.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.4.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.4.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.4.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.4.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.5.attention.self.query.weight ----- False\n",
            "Parameter: bert.encoder.layer.5.attention.self.query.bias ----- False\n",
            "Parameter: bert.encoder.layer.5.attention.self.key.weight ----- False\n",
            "Parameter: bert.encoder.layer.5.attention.self.key.bias ----- False\n",
            "Parameter: bert.encoder.layer.5.attention.self.value.weight ----- False\n",
            "Parameter: bert.encoder.layer.5.attention.self.value.bias ----- False\n",
            "Parameter: bert.encoder.layer.5.attention.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.5.attention.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.5.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.5.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.5.intermediate.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.5.intermediate.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.5.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.5.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.5.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.5.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.6.attention.self.query.weight ----- False\n",
            "Parameter: bert.encoder.layer.6.attention.self.query.bias ----- False\n",
            "Parameter: bert.encoder.layer.6.attention.self.key.weight ----- False\n",
            "Parameter: bert.encoder.layer.6.attention.self.key.bias ----- False\n",
            "Parameter: bert.encoder.layer.6.attention.self.value.weight ----- False\n",
            "Parameter: bert.encoder.layer.6.attention.self.value.bias ----- False\n",
            "Parameter: bert.encoder.layer.6.attention.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.6.attention.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.6.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.6.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.6.intermediate.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.6.intermediate.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.6.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.6.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.6.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.6.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.7.attention.self.query.weight ----- False\n",
            "Parameter: bert.encoder.layer.7.attention.self.query.bias ----- False\n",
            "Parameter: bert.encoder.layer.7.attention.self.key.weight ----- False\n",
            "Parameter: bert.encoder.layer.7.attention.self.key.bias ----- False\n",
            "Parameter: bert.encoder.layer.7.attention.self.value.weight ----- False\n",
            "Parameter: bert.encoder.layer.7.attention.self.value.bias ----- False\n",
            "Parameter: bert.encoder.layer.7.attention.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.7.attention.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.7.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.7.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.7.intermediate.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.7.intermediate.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.7.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.7.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.7.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.7.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.8.attention.self.query.weight ----- False\n",
            "Parameter: bert.encoder.layer.8.attention.self.query.bias ----- False\n",
            "Parameter: bert.encoder.layer.8.attention.self.key.weight ----- False\n",
            "Parameter: bert.encoder.layer.8.attention.self.key.bias ----- False\n",
            "Parameter: bert.encoder.layer.8.attention.self.value.weight ----- False\n",
            "Parameter: bert.encoder.layer.8.attention.self.value.bias ----- False\n",
            "Parameter: bert.encoder.layer.8.attention.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.8.attention.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.8.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.8.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.8.intermediate.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.8.intermediate.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.8.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.8.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.8.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.8.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.9.attention.self.query.weight ----- False\n",
            "Parameter: bert.encoder.layer.9.attention.self.query.bias ----- False\n",
            "Parameter: bert.encoder.layer.9.attention.self.key.weight ----- False\n",
            "Parameter: bert.encoder.layer.9.attention.self.key.bias ----- False\n",
            "Parameter: bert.encoder.layer.9.attention.self.value.weight ----- False\n",
            "Parameter: bert.encoder.layer.9.attention.self.value.bias ----- False\n",
            "Parameter: bert.encoder.layer.9.attention.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.9.attention.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.9.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.9.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.9.intermediate.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.9.intermediate.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.9.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.9.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.9.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.9.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.10.attention.self.query.weight ----- False\n",
            "Parameter: bert.encoder.layer.10.attention.self.query.bias ----- False\n",
            "Parameter: bert.encoder.layer.10.attention.self.key.weight ----- False\n",
            "Parameter: bert.encoder.layer.10.attention.self.key.bias ----- False\n",
            "Parameter: bert.encoder.layer.10.attention.self.value.weight ----- False\n",
            "Parameter: bert.encoder.layer.10.attention.self.value.bias ----- False\n",
            "Parameter: bert.encoder.layer.10.attention.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.10.attention.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.10.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.10.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.10.intermediate.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.10.intermediate.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.10.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.10.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.10.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.10.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.11.attention.self.query.weight ----- False\n",
            "Parameter: bert.encoder.layer.11.attention.self.query.bias ----- False\n",
            "Parameter: bert.encoder.layer.11.attention.self.key.weight ----- False\n",
            "Parameter: bert.encoder.layer.11.attention.self.key.bias ----- False\n",
            "Parameter: bert.encoder.layer.11.attention.self.value.weight ----- False\n",
            "Parameter: bert.encoder.layer.11.attention.self.value.bias ----- False\n",
            "Parameter: bert.encoder.layer.11.attention.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.11.attention.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.11.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.11.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.encoder.layer.11.intermediate.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.11.intermediate.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.11.output.dense.weight ----- False\n",
            "Parameter: bert.encoder.layer.11.output.dense.bias ----- False\n",
            "Parameter: bert.encoder.layer.11.output.LayerNorm.weight ----- False\n",
            "Parameter: bert.encoder.layer.11.output.LayerNorm.bias ----- False\n",
            "Parameter: bert.pooler.dense.weight ----- False\n",
            "Parameter: bert.pooler.dense.bias ----- False\n",
            "Parameter: classifier.weight ----- True\n",
            "Parameter: classifier.bias ----- True\n"
          ]
        }
      ],
      "source": [
        "# We can check whether the model was correctly updated\n",
        "for name, param in model.named_parameters():\n",
        "     print(f\"Parameter: {name} ----- {param.requires_grad}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "rVi36FJSG4ue",
        "outputId": "fbd1822a-ff7d-4983-81e8-b882a08c30c4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [157/157 01:28, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=157, training_loss=0.702758400303543, metrics={'train_runtime': 88.7689, 'train_samples_per_second': 28.163, 'train_steps_per_second': 1.769, 'total_flos': 650731195448640.0, 'train_loss': 0.702758400303543, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Trainer which executes the training process\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_test,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "eCPpixB1HCsI",
        "outputId": "2f42fbd6-9c6a-4a56-e596-1d1e1bc73f7b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32/32 00:14]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.6918076276779175,\n",
              " 'eval_f1': 0.1245674740484429,\n",
              " 'eval_runtime': 16.2766,\n",
              " 'eval_samples_per_second': 30.719,\n",
              " 'eval_steps_per_second': 1.966,\n",
              " 'epoch': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw729mLhIQL6"
      },
      "source": [
        "### Freeze blocks 1-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbsLR561Kje-",
        "outputId": "c59f9937-0916-41be-af3e-8753ff3a2f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter: 0bert.embeddings.word_embeddings.weight ----- False\n",
            "Parameter: 1bert.embeddings.position_embeddings.weight ----- False\n",
            "Parameter: 2bert.embeddings.token_type_embeddings.weight ----- False\n",
            "Parameter: 3bert.embeddings.LayerNorm.weight ----- False\n",
            "Parameter: 4bert.embeddings.LayerNorm.bias ----- False\n",
            "Parameter: 5bert.encoder.layer.0.attention.self.query.weight ----- False\n",
            "Parameter: 6bert.encoder.layer.0.attention.self.query.bias ----- False\n",
            "Parameter: 7bert.encoder.layer.0.attention.self.key.weight ----- False\n",
            "Parameter: 8bert.encoder.layer.0.attention.self.key.bias ----- False\n",
            "Parameter: 9bert.encoder.layer.0.attention.self.value.weight ----- False\n",
            "Parameter: 10bert.encoder.layer.0.attention.self.value.bias ----- False\n",
            "Parameter: 11bert.encoder.layer.0.attention.output.dense.weight ----- False\n",
            "Parameter: 12bert.encoder.layer.0.attention.output.dense.bias ----- False\n",
            "Parameter: 13bert.encoder.layer.0.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: 14bert.encoder.layer.0.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: 15bert.encoder.layer.0.intermediate.dense.weight ----- False\n",
            "Parameter: 16bert.encoder.layer.0.intermediate.dense.bias ----- False\n",
            "Parameter: 17bert.encoder.layer.0.output.dense.weight ----- False\n",
            "Parameter: 18bert.encoder.layer.0.output.dense.bias ----- False\n",
            "Parameter: 19bert.encoder.layer.0.output.LayerNorm.weight ----- False\n",
            "Parameter: 20bert.encoder.layer.0.output.LayerNorm.bias ----- False\n",
            "Parameter: 21bert.encoder.layer.1.attention.self.query.weight ----- False\n",
            "Parameter: 22bert.encoder.layer.1.attention.self.query.bias ----- False\n",
            "Parameter: 23bert.encoder.layer.1.attention.self.key.weight ----- False\n",
            "Parameter: 24bert.encoder.layer.1.attention.self.key.bias ----- False\n",
            "Parameter: 25bert.encoder.layer.1.attention.self.value.weight ----- False\n",
            "Parameter: 26bert.encoder.layer.1.attention.self.value.bias ----- False\n",
            "Parameter: 27bert.encoder.layer.1.attention.output.dense.weight ----- False\n",
            "Parameter: 28bert.encoder.layer.1.attention.output.dense.bias ----- False\n",
            "Parameter: 29bert.encoder.layer.1.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: 30bert.encoder.layer.1.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: 31bert.encoder.layer.1.intermediate.dense.weight ----- False\n",
            "Parameter: 32bert.encoder.layer.1.intermediate.dense.bias ----- False\n",
            "Parameter: 33bert.encoder.layer.1.output.dense.weight ----- False\n",
            "Parameter: 34bert.encoder.layer.1.output.dense.bias ----- False\n",
            "Parameter: 35bert.encoder.layer.1.output.LayerNorm.weight ----- False\n",
            "Parameter: 36bert.encoder.layer.1.output.LayerNorm.bias ----- False\n",
            "Parameter: 37bert.encoder.layer.2.attention.self.query.weight ----- False\n",
            "Parameter: 38bert.encoder.layer.2.attention.self.query.bias ----- False\n",
            "Parameter: 39bert.encoder.layer.2.attention.self.key.weight ----- False\n",
            "Parameter: 40bert.encoder.layer.2.attention.self.key.bias ----- False\n",
            "Parameter: 41bert.encoder.layer.2.attention.self.value.weight ----- False\n",
            "Parameter: 42bert.encoder.layer.2.attention.self.value.bias ----- False\n",
            "Parameter: 43bert.encoder.layer.2.attention.output.dense.weight ----- False\n",
            "Parameter: 44bert.encoder.layer.2.attention.output.dense.bias ----- False\n",
            "Parameter: 45bert.encoder.layer.2.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: 46bert.encoder.layer.2.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: 47bert.encoder.layer.2.intermediate.dense.weight ----- False\n",
            "Parameter: 48bert.encoder.layer.2.intermediate.dense.bias ----- False\n",
            "Parameter: 49bert.encoder.layer.2.output.dense.weight ----- False\n",
            "Parameter: 50bert.encoder.layer.2.output.dense.bias ----- False\n",
            "Parameter: 51bert.encoder.layer.2.output.LayerNorm.weight ----- False\n",
            "Parameter: 52bert.encoder.layer.2.output.LayerNorm.bias ----- False\n",
            "Parameter: 53bert.encoder.layer.3.attention.self.query.weight ----- False\n",
            "Parameter: 54bert.encoder.layer.3.attention.self.query.bias ----- False\n",
            "Parameter: 55bert.encoder.layer.3.attention.self.key.weight ----- False\n",
            "Parameter: 56bert.encoder.layer.3.attention.self.key.bias ----- False\n",
            "Parameter: 57bert.encoder.layer.3.attention.self.value.weight ----- False\n",
            "Parameter: 58bert.encoder.layer.3.attention.self.value.bias ----- False\n",
            "Parameter: 59bert.encoder.layer.3.attention.output.dense.weight ----- False\n",
            "Parameter: 60bert.encoder.layer.3.attention.output.dense.bias ----- False\n",
            "Parameter: 61bert.encoder.layer.3.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: 62bert.encoder.layer.3.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: 63bert.encoder.layer.3.intermediate.dense.weight ----- False\n",
            "Parameter: 64bert.encoder.layer.3.intermediate.dense.bias ----- False\n",
            "Parameter: 65bert.encoder.layer.3.output.dense.weight ----- False\n",
            "Parameter: 66bert.encoder.layer.3.output.dense.bias ----- False\n",
            "Parameter: 67bert.encoder.layer.3.output.LayerNorm.weight ----- False\n",
            "Parameter: 68bert.encoder.layer.3.output.LayerNorm.bias ----- False\n",
            "Parameter: 69bert.encoder.layer.4.attention.self.query.weight ----- False\n",
            "Parameter: 70bert.encoder.layer.4.attention.self.query.bias ----- False\n",
            "Parameter: 71bert.encoder.layer.4.attention.self.key.weight ----- False\n",
            "Parameter: 72bert.encoder.layer.4.attention.self.key.bias ----- False\n",
            "Parameter: 73bert.encoder.layer.4.attention.self.value.weight ----- False\n",
            "Parameter: 74bert.encoder.layer.4.attention.self.value.bias ----- False\n",
            "Parameter: 75bert.encoder.layer.4.attention.output.dense.weight ----- False\n",
            "Parameter: 76bert.encoder.layer.4.attention.output.dense.bias ----- False\n",
            "Parameter: 77bert.encoder.layer.4.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: 78bert.encoder.layer.4.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: 79bert.encoder.layer.4.intermediate.dense.weight ----- False\n",
            "Parameter: 80bert.encoder.layer.4.intermediate.dense.bias ----- False\n",
            "Parameter: 81bert.encoder.layer.4.output.dense.weight ----- False\n",
            "Parameter: 82bert.encoder.layer.4.output.dense.bias ----- False\n",
            "Parameter: 83bert.encoder.layer.4.output.LayerNorm.weight ----- False\n",
            "Parameter: 84bert.encoder.layer.4.output.LayerNorm.bias ----- False\n",
            "Parameter: 85bert.encoder.layer.5.attention.self.query.weight ----- False\n",
            "Parameter: 86bert.encoder.layer.5.attention.self.query.bias ----- False\n",
            "Parameter: 87bert.encoder.layer.5.attention.self.key.weight ----- False\n",
            "Parameter: 88bert.encoder.layer.5.attention.self.key.bias ----- False\n",
            "Parameter: 89bert.encoder.layer.5.attention.self.value.weight ----- False\n",
            "Parameter: 90bert.encoder.layer.5.attention.self.value.bias ----- False\n",
            "Parameter: 91bert.encoder.layer.5.attention.output.dense.weight ----- False\n",
            "Parameter: 92bert.encoder.layer.5.attention.output.dense.bias ----- False\n",
            "Parameter: 93bert.encoder.layer.5.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: 94bert.encoder.layer.5.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: 95bert.encoder.layer.5.intermediate.dense.weight ----- False\n",
            "Parameter: 96bert.encoder.layer.5.intermediate.dense.bias ----- False\n",
            "Parameter: 97bert.encoder.layer.5.output.dense.weight ----- False\n",
            "Parameter: 98bert.encoder.layer.5.output.dense.bias ----- False\n",
            "Parameter: 99bert.encoder.layer.5.output.LayerNorm.weight ----- False\n",
            "Parameter: 100bert.encoder.layer.5.output.LayerNorm.bias ----- False\n",
            "Parameter: 101bert.encoder.layer.6.attention.self.query.weight ----- False\n",
            "Parameter: 102bert.encoder.layer.6.attention.self.query.bias ----- False\n",
            "Parameter: 103bert.encoder.layer.6.attention.self.key.weight ----- False\n",
            "Parameter: 104bert.encoder.layer.6.attention.self.key.bias ----- False\n",
            "Parameter: 105bert.encoder.layer.6.attention.self.value.weight ----- False\n",
            "Parameter: 106bert.encoder.layer.6.attention.self.value.bias ----- False\n",
            "Parameter: 107bert.encoder.layer.6.attention.output.dense.weight ----- False\n",
            "Parameter: 108bert.encoder.layer.6.attention.output.dense.bias ----- False\n",
            "Parameter: 109bert.encoder.layer.6.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: 110bert.encoder.layer.6.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: 111bert.encoder.layer.6.intermediate.dense.weight ----- False\n",
            "Parameter: 112bert.encoder.layer.6.intermediate.dense.bias ----- False\n",
            "Parameter: 113bert.encoder.layer.6.output.dense.weight ----- False\n",
            "Parameter: 114bert.encoder.layer.6.output.dense.bias ----- False\n",
            "Parameter: 115bert.encoder.layer.6.output.LayerNorm.weight ----- False\n",
            "Parameter: 116bert.encoder.layer.6.output.LayerNorm.bias ----- False\n",
            "Parameter: 117bert.encoder.layer.7.attention.self.query.weight ----- False\n",
            "Parameter: 118bert.encoder.layer.7.attention.self.query.bias ----- False\n",
            "Parameter: 119bert.encoder.layer.7.attention.self.key.weight ----- False\n",
            "Parameter: 120bert.encoder.layer.7.attention.self.key.bias ----- False\n",
            "Parameter: 121bert.encoder.layer.7.attention.self.value.weight ----- False\n",
            "Parameter: 122bert.encoder.layer.7.attention.self.value.bias ----- False\n",
            "Parameter: 123bert.encoder.layer.7.attention.output.dense.weight ----- False\n",
            "Parameter: 124bert.encoder.layer.7.attention.output.dense.bias ----- False\n",
            "Parameter: 125bert.encoder.layer.7.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: 126bert.encoder.layer.7.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: 127bert.encoder.layer.7.intermediate.dense.weight ----- False\n",
            "Parameter: 128bert.encoder.layer.7.intermediate.dense.bias ----- False\n",
            "Parameter: 129bert.encoder.layer.7.output.dense.weight ----- False\n",
            "Parameter: 130bert.encoder.layer.7.output.dense.bias ----- False\n",
            "Parameter: 131bert.encoder.layer.7.output.LayerNorm.weight ----- False\n",
            "Parameter: 132bert.encoder.layer.7.output.LayerNorm.bias ----- False\n",
            "Parameter: 133bert.encoder.layer.8.attention.self.query.weight ----- False\n",
            "Parameter: 134bert.encoder.layer.8.attention.self.query.bias ----- False\n",
            "Parameter: 135bert.encoder.layer.8.attention.self.key.weight ----- False\n",
            "Parameter: 136bert.encoder.layer.8.attention.self.key.bias ----- False\n",
            "Parameter: 137bert.encoder.layer.8.attention.self.value.weight ----- False\n",
            "Parameter: 138bert.encoder.layer.8.attention.self.value.bias ----- False\n",
            "Parameter: 139bert.encoder.layer.8.attention.output.dense.weight ----- False\n",
            "Parameter: 140bert.encoder.layer.8.attention.output.dense.bias ----- False\n",
            "Parameter: 141bert.encoder.layer.8.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: 142bert.encoder.layer.8.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: 143bert.encoder.layer.8.intermediate.dense.weight ----- False\n",
            "Parameter: 144bert.encoder.layer.8.intermediate.dense.bias ----- False\n",
            "Parameter: 145bert.encoder.layer.8.output.dense.weight ----- False\n",
            "Parameter: 146bert.encoder.layer.8.output.dense.bias ----- False\n",
            "Parameter: 147bert.encoder.layer.8.output.LayerNorm.weight ----- False\n",
            "Parameter: 148bert.encoder.layer.8.output.LayerNorm.bias ----- False\n",
            "Parameter: 149bert.encoder.layer.9.attention.self.query.weight ----- False\n",
            "Parameter: 150bert.encoder.layer.9.attention.self.query.bias ----- False\n",
            "Parameter: 151bert.encoder.layer.9.attention.self.key.weight ----- False\n",
            "Parameter: 152bert.encoder.layer.9.attention.self.key.bias ----- False\n",
            "Parameter: 153bert.encoder.layer.9.attention.self.value.weight ----- False\n",
            "Parameter: 154bert.encoder.layer.9.attention.self.value.bias ----- False\n",
            "Parameter: 155bert.encoder.layer.9.attention.output.dense.weight ----- False\n",
            "Parameter: 156bert.encoder.layer.9.attention.output.dense.bias ----- False\n",
            "Parameter: 157bert.encoder.layer.9.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: 158bert.encoder.layer.9.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: 159bert.encoder.layer.9.intermediate.dense.weight ----- False\n",
            "Parameter: 160bert.encoder.layer.9.intermediate.dense.bias ----- False\n",
            "Parameter: 161bert.encoder.layer.9.output.dense.weight ----- False\n",
            "Parameter: 162bert.encoder.layer.9.output.dense.bias ----- False\n",
            "Parameter: 163bert.encoder.layer.9.output.LayerNorm.weight ----- False\n",
            "Parameter: 164bert.encoder.layer.9.output.LayerNorm.bias ----- False\n",
            "Parameter: 165bert.encoder.layer.10.attention.self.query.weight ----- False\n",
            "Parameter: 166bert.encoder.layer.10.attention.self.query.bias ----- False\n",
            "Parameter: 167bert.encoder.layer.10.attention.self.key.weight ----- False\n",
            "Parameter: 168bert.encoder.layer.10.attention.self.key.bias ----- False\n",
            "Parameter: 169bert.encoder.layer.10.attention.self.value.weight ----- False\n",
            "Parameter: 170bert.encoder.layer.10.attention.self.value.bias ----- False\n",
            "Parameter: 171bert.encoder.layer.10.attention.output.dense.weight ----- False\n",
            "Parameter: 172bert.encoder.layer.10.attention.output.dense.bias ----- False\n",
            "Parameter: 173bert.encoder.layer.10.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: 174bert.encoder.layer.10.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: 175bert.encoder.layer.10.intermediate.dense.weight ----- False\n",
            "Parameter: 176bert.encoder.layer.10.intermediate.dense.bias ----- False\n",
            "Parameter: 177bert.encoder.layer.10.output.dense.weight ----- False\n",
            "Parameter: 178bert.encoder.layer.10.output.dense.bias ----- False\n",
            "Parameter: 179bert.encoder.layer.10.output.LayerNorm.weight ----- False\n",
            "Parameter: 180bert.encoder.layer.10.output.LayerNorm.bias ----- False\n",
            "Parameter: 181bert.encoder.layer.11.attention.self.query.weight ----- False\n",
            "Parameter: 182bert.encoder.layer.11.attention.self.query.bias ----- False\n",
            "Parameter: 183bert.encoder.layer.11.attention.self.key.weight ----- False\n",
            "Parameter: 184bert.encoder.layer.11.attention.self.key.bias ----- False\n",
            "Parameter: 185bert.encoder.layer.11.attention.self.value.weight ----- False\n",
            "Parameter: 186bert.encoder.layer.11.attention.self.value.bias ----- False\n",
            "Parameter: 187bert.encoder.layer.11.attention.output.dense.weight ----- False\n",
            "Parameter: 188bert.encoder.layer.11.attention.output.dense.bias ----- False\n",
            "Parameter: 189bert.encoder.layer.11.attention.output.LayerNorm.weight ----- False\n",
            "Parameter: 190bert.encoder.layer.11.attention.output.LayerNorm.bias ----- False\n",
            "Parameter: 191bert.encoder.layer.11.intermediate.dense.weight ----- False\n",
            "Parameter: 192bert.encoder.layer.11.intermediate.dense.bias ----- False\n",
            "Parameter: 193bert.encoder.layer.11.output.dense.weight ----- False\n",
            "Parameter: 194bert.encoder.layer.11.output.dense.bias ----- False\n",
            "Parameter: 195bert.encoder.layer.11.output.LayerNorm.weight ----- False\n",
            "Parameter: 196bert.encoder.layer.11.output.LayerNorm.bias ----- False\n",
            "Parameter: 197bert.pooler.dense.weight ----- False\n",
            "Parameter: 198bert.pooler.dense.bias ----- False\n",
            "Parameter: 199classifier.weight ----- True\n",
            "Parameter: 200classifier.bias ----- True\n"
          ]
        }
      ],
      "source": [
        "# We can check whether the model was correctly updated\n",
        "for index, (name, param) in enumerate(model.named_parameters()):\n",
        "     print(f\"Parameter: {index}{name} ----- {param.requires_grad}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "QyleqOHICBjj",
        "outputId": "1dde97cc-9d86-4f90-ea7c-a41329a671c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [157/157 01:59, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32/32 00:14]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.5053647756576538,\n",
              " 'eval_f1': 0.7467811158798283,\n",
              " 'eval_runtime': 16.1459,\n",
              " 'eval_samples_per_second': 30.968,\n",
              " 'eval_steps_per_second': 1.982,\n",
              " 'epoch': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# Load model\n",
        "model_id = \"bert-base-cased\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Encoder block 10 starts at index 165 and\n",
        "# we freeze everything before that block\n",
        "for index, (name, param) in enumerate(model.named_parameters()):\n",
        "    if index < 165:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Trainer which executes the training process\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_test,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAfBhqVwC61e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f9443e06b1814fdd8004be5a7dcab9ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f45b1eb227234f4598a14fd5d39075a7",
              "IPY_MODEL_7cb9b6a69a984264b6bf7a63930ae68f",
              "IPY_MODEL_1f6bc5e3d50142bd9a784d8797263874"
            ],
            "layout": "IPY_MODEL_fc5528b82a174aa4810718065c60f4d4"
          }
        },
        "f45b1eb227234f4598a14fd5d39075a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80be597bdad24be9987fdfc69ceb6e70",
            "placeholder": "​",
            "style": "IPY_MODEL_00d72f8f1a744da287fe1fecc4ada467",
            "value": "Map: 100%"
          }
        },
        "7cb9b6a69a984264b6bf7a63930ae68f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e493847df8444279ee2a619e906ee26",
            "max": 2500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d04c144e7659440ab7c3a65423c63fb0",
            "value": 2500
          }
        },
        "1f6bc5e3d50142bd9a784d8797263874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67e11a8b1b654b4ab589df09407bf144",
            "placeholder": "​",
            "style": "IPY_MODEL_b5dd60026fb24dd38b55b2af541e3572",
            "value": " 2500/2500 [00:02&lt;00:00, 870.83 examples/s]"
          }
        },
        "fc5528b82a174aa4810718065c60f4d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80be597bdad24be9987fdfc69ceb6e70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00d72f8f1a744da287fe1fecc4ada467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e493847df8444279ee2a619e906ee26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d04c144e7659440ab7c3a65423c63fb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67e11a8b1b654b4ab589df09407bf144": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5dd60026fb24dd38b55b2af541e3572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "240f76c17ddd4b55b5d428cc9c367041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1cf9f6b247dd49488af3e1b31b46a9c6",
              "IPY_MODEL_6e8e8218551c4e1abf372901414a9bf1",
              "IPY_MODEL_5c6e3083b5624b31a8b6dfaf2bc3aaa5"
            ],
            "layout": "IPY_MODEL_a6f42dff14d8461e8089f59cbd7ea2ee"
          }
        },
        "1cf9f6b247dd49488af3e1b31b46a9c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4db9776640e54cedbcb80bc46bc18c70",
            "placeholder": "​",
            "style": "IPY_MODEL_b7205c659ef1463ba3c5f809917912a1",
            "value": "Map: 100%"
          }
        },
        "6e8e8218551c4e1abf372901414a9bf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed26c166f97a49a49bb9c148c0caf261",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5fb29afb81244a668bd8c61770f36cee",
            "value": 500
          }
        },
        "5c6e3083b5624b31a8b6dfaf2bc3aaa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a16735f95294188837d05af014e4393",
            "placeholder": "​",
            "style": "IPY_MODEL_d43e5cbf5af54e2186ed00ec3c461dae",
            "value": " 500/500 [00:00&lt;00:00, 826.57 examples/s]"
          }
        },
        "a6f42dff14d8461e8089f59cbd7ea2ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4db9776640e54cedbcb80bc46bc18c70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7205c659ef1463ba3c5f809917912a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed26c166f97a49a49bb9c148c0caf261": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fb29afb81244a668bd8c61770f36cee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a16735f95294188837d05af014e4393": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d43e5cbf5af54e2186ed00ec3c461dae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}