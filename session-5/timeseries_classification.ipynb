{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107/blob/main/session-5/timeseries_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_7gX2-42Aij"
      },
      "source": [
        "# Timeseries classification using LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgESH_cN2Aiq"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this lab exercise, we will apply a simple LSTM to do timeseries classification.\n",
        "\n",
        "*The lab is adapted from the example codes on keras.io*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VM-tytE2Air"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQbJgKCA2Ais",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKij8ezY2Aiv"
      },
      "source": [
        "## Load the data: the FordA dataset\n",
        "\n",
        "### Dataset description\n",
        "\n",
        "The dataset we are using here is called FordA.\n",
        "The data comes from the UCR archive.\n",
        "The dataset contains 3601 training instances and another 1320 testing instances.\n",
        "Each timeseries corresponds to a measurement of engine noise captured by a motor sensor.\n",
        "For this task, the goal is to automatically detect the presence of a specific issue with\n",
        "the engine. The problem is a balanced binary classification task. The full description of\n",
        "this dataset can be found [here](http://www.j-wichard.de/publications/FordPaper.pdf).\n",
        "\n",
        "### Read the data\n",
        "\n",
        "We will use the `FordA_TRAIN` file for training and the\n",
        "`FordA_TEST` file for testing. The simplicity of this dataset\n",
        "allows us to demonstrate effectively how to use LSTM for timeseries classification.\n",
        "In this file, the first column corresponds to the label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "1ZawJg9YV5H5"
      },
      "outputs": [],
      "source": [
        "train_data_url = 'https://raw.githubusercontent.com/nyp-sit/iti107/main/session-5/FordA_TRAIN.txt'\n",
        "train_df = pd.read_csv(train_data_url, delim_whitespace=True, header=None)\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IuGfKsD2kdL",
        "tags": []
      },
      "outputs": [],
      "source": [
        "test_data_url = 'https://raw.githubusercontent.com/nyp-sit/iti107/main/session-5/FordA_TEST.txt'\n",
        "test_df = pd.read_csv(test_data_url, delim_whitespace=True, header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_f-0D_V2Aiw",
        "tags": []
      },
      "outputs": [],
      "source": [
        "x_train, y_train = train_df.loc[:,1:].values, train_df.loc[:,0].values\n",
        "x_test, y_test = test_df.loc[:,1:].values, test_df.loc[:,0].values\n",
        "# x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CXDhRKf2UJ6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7IQMTNm2Aix"
      },
      "source": [
        "## Visualize the data\n",
        "\n",
        "Here we visualize one timeseries example for each class in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXLq6Tsx2Aix",
        "tags": []
      },
      "outputs": [],
      "source": [
        "classes = np.unique(np.concatenate((y_train, y_test), axis=0))\n",
        "\n",
        "plt.figure()\n",
        "for c in classes:\n",
        "    c_x_train = x_train[y_train == c]\n",
        "    plt.plot(c_x_train[0], label=\"class \" + str(c))\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPfcTJ0E2Aiy"
      },
      "source": [
        "## Standardize the data\n",
        "\n",
        "Our timeseries are already in a single length (500). However, their values are\n",
        "usually in various ranges. This is not ideal for a neural network;\n",
        "in general we should seek to make the input values normalized.\n",
        "For this specific dataset, the data is already z-normalized: each timeseries sample\n",
        "has a mean equal to zero and a standard deviation equal to one.\n",
        "\n",
        "Note that the timeseries data used here are univariate, meaning we only have one channel\n",
        "per timeseries example.\n",
        "We will therefore transform the timeseries into a multivariate one with one channel\n",
        "using a simple reshaping via numpy.\n",
        "This will allow us to construct a model that is easily applicable to multivariate time\n",
        "series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ebufr_82Aiz",
        "tags": []
      },
      "outputs": [],
      "source": [
        "x_train = np.expand_dims(x_train, axis=2)\n",
        "x_test = np.expand_dims(x_test, axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2v4lzADf4jqw",
        "tags": []
      },
      "outputs": [],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5W-H8xP2Ai0"
      },
      "source": [
        "Finally, in order to use `sparse_categorical_crossentropy`, we will have to count\n",
        "the number of classes beforehand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhcf8Xs_2Ai0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "num_classes = len(np.unique(y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTKf9rB12Ai0"
      },
      "source": [
        "Now we shuffle the training set because we will be using the `validation_split` option\n",
        "later when training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBNsku_Y2Ai1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "idx = np.random.permutation(len(x_train))\n",
        "x_train = x_train[idx]\n",
        "y_train = y_train[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOQxD5sG2Ai1"
      },
      "source": [
        "Standardize the labels to positive integers.\n",
        "The expected labels will then be 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MytAaFOL2Ai1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "y_train[y_train == -1] = 0\n",
        "y_test[y_test == -1] = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ARwUSP52Ai2"
      },
      "source": [
        "## Build a model\n",
        "\n",
        "We use a single LSTM layer to capture the temporal information and return the hidden at each timestep.  We then feed these timesteps into the dense layers for classification.\n",
        "\n",
        "Note that we set the `return_sequences=True` to return the hidden states at every time-step. The output shape is thus of 3D shape (batch, time-steps, feature). To apply Dense layer to every time step, we use keras TimeDistributed wrapper.  To connect to final Dense layer, we need to Flatten this to a 2D shape (batch, features)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxFvd6Ng2Ai2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def make_model(input_shape):\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "    x = keras.layers.LSTM(32, return_sequences=True)(input_layer)\n",
        "    x = keras.layers.LSTM(32, return_sequences=True)(x)\n",
        "    x = keras.layers.TimeDistributed(keras.layers.Dense(16, activation='relu'))(x)\n",
        "    x = keras.layers.Dropout(0.4)(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "model = make_model(input_shape=x_train.shape[1:])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAs2PPCv2Ai2"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzRLPbM52Ai3",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "epochs = 250\n",
        "batch_size = 256\n",
        "\n",
        "def create_tb_callback():\n",
        "\n",
        "    root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
        "\n",
        "    def get_run_logdir():    # use a new directory for each run\n",
        "        import time\n",
        "        run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "        return os.path.join(root_logdir, run_id)\n",
        "\n",
        "    run_logdir = get_run_logdir()\n",
        "\n",
        "    tb_callback = keras.callbacks.TensorBoard(run_logdir)\n",
        "\n",
        "    return tb_callback\n",
        "\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_accuracy\", patience=50, restore_best_weights=True\n",
        "    ),\n",
        "#     keras.callbacks.ModelCheckpoint(\n",
        "#         \"best_model\", save_best_only=True, monitor=\"val_accuracy\"\n",
        "#     ),\n",
        "\n",
        "    create_tb_callback()\n",
        "]\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    validation_split=0.2,\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9viocqIp2Ai3"
      },
      "source": [
        "## Evaluate model on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpnx7E3u40Eg"
      },
      "outputs": [],
      "source": [
        "# model = keras.models.load_model(\"best_model\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "print(\"Test accuracy\", test_acc)\n",
        "print(\"Test loss\", test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfyeiRLM2Ai3"
      },
      "source": [
        "## Visualize training using Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amK-p6U12Ai4"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "%tensorboard --logdir tb_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DW-SDUr2Ai4"
      },
      "source": [
        "We can see how the training accuracy reaches almost 1 after 100 epochs.\n",
        "However, the validation accuracy is stuck at around 0.88. The model is clearly overfitting. Try experimenting with other regularization methods such as L1/L2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWVNz4x5V5H9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "timeseries_classification_from_scratch",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}