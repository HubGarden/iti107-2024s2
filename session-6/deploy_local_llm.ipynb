{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhhm4xqxuKbLtHCQIXK1fF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107-2024s2/blob/main/session-6/deploy_local_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy Custom LLM Model locally\n",
        "\n",
        "In this exercise, you will learn how to:\n",
        "\n",
        "1. convert the trained model to GGUF format\n",
        "2. optimize the model for fast inference through quantization\n",
        "3. deploy the model locally"
      ],
      "metadata": {
        "id": "9Sb2yUORDRkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the Model\n",
        "\n",
        "We will first download the model from Hugging face using huggingface-cli.\n"
      ],
      "metadata": {
        "id": "T8oATAZLFMk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install \"huggingface_hub[cli]\""
      ],
      "metadata": {
        "id": "yL8AqaDjFWow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## we download our model hosted on huggingface, to a local directory called Llama-3.2-1B-chat-doctor\n",
        "## replace the model repo ID with your own\n",
        "%%capture\n",
        "!huggingface-cli download --local-dir Llama-3.2-1B-chat-doctor khengkok/Llama-3.2-1B-chat-doctor"
      ],
      "metadata": {
        "id": "N64AUflPFeoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert the Model to GGUF\n",
        "\n",
        "GGUF is a binary format that is optimized for quick loading and saving of models, making it highly efficient for inference purposes.\n",
        "\n",
        "`llama.cpp` is an open source software library that performs inference on various large language models such as Llama. It also contains a set of utility to convert model to GGUF and also perform quantization using different quantization schemes."
      ],
      "metadata": {
        "id": "EgAPdx6bEEty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup llama.cpp\n",
        "\n",
        "We will clone the git repo of llamma.cpp and use the python script to convert the model.\n",
        "we will also compile the llama.cpp to get the various utility module, one of which is to perform quantization."
      ],
      "metadata": {
        "id": "tuUYkpVgElsT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8lRQqO4it-j"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "git clone --depth=1 https://github.com/ggerganov/llama.cpp.git\n",
        "cd llama.cpp\n",
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now run the script `convert_hf_to_gguf` found inside the llama.cpp directory to conver the model that we downloaded into gguf format."
      ],
      "metadata": {
        "id": "BK-nYs_qGEtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd llama.cpp\n",
        "python convert_hf_to_gguf.py --outfile /content/Llama-3.2-1B-chat-doctor.gguf /content/Llama-3.2-1B-chat-doctor"
      ],
      "metadata": {
        "id": "8hoJ2VKWl7tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantize the model\n",
        "\n",
        "We will quantize the model using Q4_K_M schema.  For more information about the different quantization schemas, you can refer to this [document](https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/README.md).\n",
        "\n",
        "We can use the llama-quantize utility to do this. However, you will need to build the llama.cpp to get this utility, by running the Makefile inside llama.cpp. The build will take quite a while."
      ],
      "metadata": {
        "id": "PbO8kvjvHbe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## build the llama.cpp\n",
        "%%bash\n",
        "cd llama.cpp\n",
        "make"
      ],
      "metadata": {
        "id": "ArDZYgpqIajw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we run the llama-quantize, selecting Q4_K_M as the quantization scheme, to quantize our gguf. We will name our quantized model as `Llama-3.2-1B-chat-doctor-Q4_K_M.gguf` to differentiate from the original model."
      ],
      "metadata": {
        "id": "UTunKxRdIqn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "./llama.cpp/llama-quantize /content/Llama-3.2-1B-chat-doctor.gguf  /content/Llama-3.2-1B-chat-doctor-Q4_K_M.gguf Q4_K_M"
      ],
      "metadata": {
        "id": "30FU-0VFrrKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload the model to Hugging Face\n",
        "\n",
        "You will need your HF access token to login to the Hugging Face in order to upload your model."
      ],
      "metadata": {
        "id": "K9pFwAx7JDGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "bs5bUO53uBYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we choose to upload our file to our repo `khengkok/Llama-3.2-1b-chat-doctor` which is hosting the original model. We use huggingface api to upload the file."
      ],
      "metadata": {
        "id": "smcoI2F3JTtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"/content/Llama-3.2-1B-chat-doctor-Q4_K_M.gguf\",\n",
        "    path_in_repo=\"Llama-3.2-1B-chat-doctor-Q4_K_M.gguf\",\n",
        "    repo_id=\"khengkok/Llama-3.2-1b-chat-doctor\",\n",
        "    repo_type=\"model\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "1VYgTfX6s6Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the model using Ollama   \n",
        "\n",
        "Ollama is a very popular platform to run your local LLM. It exposes OpenAI compatible API, so you can easily migrate your existing applications built for OpenAI easily to Ollama-hosted models\n",
        "\n",
        "\n",
        "### Installation\n",
        "\n",
        "Follow the instructions [here](https://github.com/ollama/ollama/tree/main) for installation for MacOS, Linux and Windows.\n",
        "\n",
        "\n",
        "### Download your model\n",
        "\n",
        "1. Download your gguf model into a folder of your local PC, say /home/ubuntu/models\n",
        "\n",
        "2. Create a Modelfile in the same folder with the following content:\n",
        "\n",
        "```\n",
        "FROM ./Llama-3.2-1b-chat-doctor-Q4_K_M.gguf\n",
        "TEMPLATE \"\"\"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{{ .Response }}<|eot_id|>\"\"\"\n",
        "PARAMETER stop \"<|start_header_id|>\"\n",
        "PARAMETER stop \"<|end_header_id|>\"\n",
        "PARAMETER stop \"<|eot_id|>\"\n",
        "PARAMETER stop \"<|reserved_special_token\"\n",
        "```\n",
        "\n",
        "You can also set the temperature, top-K, etc parameters in the Modelfile. You can refer to this [link](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)  for Modelfile documentation\n",
        "\n",
        "3. Run your model using the following commands:\n",
        "\n",
        "```\n",
        "ollama create chatdoctor -f ./Modelfile\n",
        "ollama run chatdoctor\n",
        "```\n",
        "\n",
        "and you will see a prompt, that let you interact with your model:\n",
        "\n",
        "```\n",
        ">>Hi doctor, I have stomach pain.\n",
        "```\n",
        "\n",
        "You can stop the model by typing:\n",
        "\n",
        "```\n",
        "ollama stop chatdoctor\n",
        "```\n",
        "\n",
        "4. If you want to run the model as a server instead of the shell:\n",
        "\n",
        "```\n",
        "./ollama serve\n",
        "\n",
        "and in a separate shell, run a model:\n",
        "\n",
        "./ollama run chatdoctor\n",
        "\n",
        "```\n",
        "\n",
        "5. Call API\n",
        "\n",
        "You can interact with the model using OpenAI Rest API:\n",
        "\n",
        "```\n",
        "curl http://localhost:11434/v1/chat/completions \\\n",
        "-d '{\n",
        "  \"model\": \"chatdoctor\",\n",
        "  \"messages\": [\n",
        "    { \"role\": \"user\", \"content\": \"Hi, doctor I have stomach pain.\" }\n",
        "  ]\n",
        "}'\n",
        "```\n",
        "\n",
        "You can use OpenAI python library to call the endpoint too, e.g.\n",
        "\n",
        "```\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url = 'http://localhost:11434/v1',\n",
        "    api_key='ollama', # required, but unused\n",
        ")\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"chatdoctor\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": \"Hi doctor, I have stomach ache.\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "xY5qTCPn1Cw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other GUI application to run the model\n",
        "\n",
        "There are quite a number of GUI application you can use to run your model. These GUI application presents a chat interface similar to what is available from ChatGPT.  Some examples are:\n",
        "1. [Jan](https://jan.ai/)\n",
        "2. [LM Studio](https://lmstudio.ai/)\n",
        "3. [GPT4All](https://www.nomic.ai/gpt4all)"
      ],
      "metadata": {
        "id": "ObBTwGIGYkvs"
      }
    }
  ]
}