{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "word_embedding_glove.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107-2024S2/blob/main/session-4/word_embedding_glove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SN5USFEIIK3"
      },
      "source": [
        "# Using Pre-trained Contex-free Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6mJg1g3apaz"
      },
      "source": [
        "In this lab exercise, we will use a pretrained word embedding (GloVE) for our text classification task, instead of training our own embedding layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZUQErGewZxE"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RutaI-Tpev3T"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBFctV8-JZOc"
      },
      "source": [
        "### Download the IMDb Dataset\n",
        "\n",
        "Download the dataset using Keras file utility and do the clean-up as before"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloaded the datasets.\n",
        "train_data_url = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_train.csv'\n",
        "test_data_url = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_test.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_data_url)\n",
        "test_df = pd.read_csv(test_data_url)"
      ],
      "metadata": {
        "id": "EfqH11_tttLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will just use a subset of 10000 for training/validation and 500 for testing."
      ],
      "metadata": {
        "id": "SsWab00VAZlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_SIZE = 10000\n",
        "TEST_SIZE = 500\n",
        "BATCH_SIZE = 1000\n",
        "\n",
        "train_df = train_df.sample(n=TRAIN_SIZE, random_state=128)\n",
        "test_df = test_df.sample(n=TEST_SIZE, random_state=128)\n",
        "\n",
        "# convert the text label to numeric label\n",
        "train_df['sentiment'] =  train_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n",
        "test_df['sentiment'] =  test_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)"
      ],
      "metadata": {
        "id": "516f73OI0w7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=128)"
      ],
      "metadata": {
        "id": "d2Xrb3WICK9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's convert to pandas dataframe to Tensorflow Dataset (tf.data.Dataset) suitable for model training later."
      ],
      "metadata": {
        "id": "6ZZtrMn8BhpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "            (train_df['review'].values,\n",
        "            train_df['sentiment'].values)\n",
        ")\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices(\n",
        "            (val_df['review'].values,\n",
        "            val_df['sentiment'].values)\n",
        ")\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(\n",
        "            (test_df['review'].values,\n",
        "            test_df['sentiment'].values)\n",
        ")\n",
        "\n",
        "# optimize the data pipeline\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "llNIKEuI4FxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_df)"
      ],
      "metadata": {
        "id": "pSHzmq9iEBDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGicgV5qT0wh"
      },
      "source": [
        "## Text preprocessing\n",
        "\n",
        "We then initialize a TextVectorization layer with the desired parameters to vectorize our movie reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MlsXzo-ZlfK"
      },
      "source": [
        "# Vocabulary size and number of words in a sequence.\n",
        "VOCAB_SIZE = 10000\n",
        "MAX_SEQUENCE_LENGTH = 500\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers.\n",
        "# Set output_sequence length as all samples are not of the same length.\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "text_ds = train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqBazMiVQkj1"
      },
      "source": [
        "## Getting the pre-trained embedding model\n",
        "\n",
        "We will use the pre-trained GloVe embeddings available from [stanford site](https://nlp.stanford.edu/projects/glove/). The original zip file contains embedding of different dimensions (e.g. 50d, 100d, 200d, etc) and is more than 800MB in file size. To save downloading time, we have made a copy of 50d GloVe file available on our course website for download. If you want to experiment with other embedding dimensions, please feel free to download from the stanford site.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRh8JKK8JIQ-"
      },
      "source": [
        "glove_url = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/pretrained-models/glove.6B.50d.zip'\n",
        "\n",
        "glove_files = tf.keras.utils.get_file(\"glove.6B.50d.zip\", glove_url,\n",
        "                                    extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXg9X3iZOAQc"
      },
      "source": [
        "## Load the Embedding layer\n",
        "\n",
        "In the code below, we read the embeddings from the download file line by line to create the embeddings index and then initialize the Keras embedding layer with this embeddings index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJZYHMjt963-"
      },
      "source": [
        "# Load up the GloVe word embedding data\n",
        "EMBEDDING_DIM = 50\n",
        "print(\"Loading GloVe Word Embedding...\")\n",
        "embeddings_index = {}\n",
        "\n",
        "with open('glove.6B.50d.txt', encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTLmbU9ePM1I"
      },
      "source": [
        "Let's print out the embedding for the word `happy`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYGK9k5nO_eD"
      },
      "source": [
        "embeddings_index['happy']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oZPVhvt-QPn"
      },
      "source": [
        "# Construct the word embedding matrix that will be used in the Embedding layer.\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "glove_embedding_matrix = np.zeros((len(vocab), EMBEDDING_DIM))\n",
        "for i, word in enumerate(vocab):\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        glove_embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-JSrknYDVSF"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_layer = tf.keras.layers.Embedding(VOCAB_SIZE,\n",
        "                            EMBEDDING_DIM,\n",
        "                            input_length = MAX_SEQUENCE_LENGTH,\n",
        "                            weights=[glove_embedding_matrix],\n",
        "                            trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "OjJKWC3T8clf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI9_wLIiWO8Z"
      },
      "source": [
        "## Create a classification model\n",
        "\n",
        "We will now create the model as before, but this time with embedding layer initialized with pretrained embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHLcFtn5Wsqj"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    vectorize_layer,\n",
        "    embedding_layer,\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjLNgKO7W2fe"
      },
      "source": [
        "## Compile and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4Hg3IHFt4Px"
      },
      "source": [
        "import os\n",
        "root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
        "\n",
        "def get_run_logdir():    # use a new directory for each run\n",
        "\timport time\n",
        "\trun_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "\treturn os.path.join(root_logdir, run_id)\n",
        "\n",
        "run_logdir = get_run_logdir()\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=run_logdir)\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"bestcheckpoint.weights.h5\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OrKAKAKIbuH"
      },
      "source": [
        "Compile and train the model using the `Adam` optimizer and `BinaryCrossentropy` loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCUgdP69Wzix"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mQehiQyv8rP"
      },
      "source": [
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=100,\n",
        "    callbacks=[tensorboard_callback, model_checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apYpmmcC7tTY"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tb_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wYnVedSPfmX"
      },
      "source": [
        "With pretrained embedding layer, we reaches a validation accuracy of around 70%, worse than jointly train our embedding layer with the classification task.\n",
        "\n",
        "This maybe because the kind of vocabulary used to train the pretrained embedding is quite different from the one used in the IMDB dataset.\n",
        "\n",
        "If we have enough data (like in our case), joinly train our own embedding layer will usually yield a better performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTJMGvvR6mTs"
      },
      "source": [
        "Let's evaluate the model on our test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk-tEK086qko"
      },
      "source": [
        "model.load_weights(\"bestcheckpoint.weights.h5\")\n",
        "model.evaluate(test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbpkf20ars57"
      },
      "source": [
        "## Additional Exercise\n",
        "\n",
        "You can try other pretrained embedding available from the GloVe website, such as those trained on Common Crawl dataset (Beware: this pretrained embedding is huge, i.e. more than 1.7GB)"
      ]
    }
  ]
}