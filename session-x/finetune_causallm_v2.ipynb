{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd00279-a7c5-493c-bd99-6e183f4c4379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install transformers sentencepiece datasets accelerate bitsandbytes scipy rouge_score peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd592d-0ad0-4f1b-94f4-0e5ae9dbeec7",
   "metadata": {},
   "source": [
    "# Fine-Tune a Causal Language Model for Dialogue Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976f3255-166f-4e6f-aaf0-ef6c4703b9f7",
   "metadata": {},
   "source": [
    "In this exercise, you will fine-tune Meta's Llama 2 for enhanced dialogue summarization. Llama 2 is a large language model (LLM) free for research and commercial use. It is one of the top-performing open-source LLM  comparable to GPT-3.5 on several benchmarks. \n",
    "\n",
    "We will explore the use of Parameter Efficient Fine-Tuning (PEFT) for fine-tuning, and evaluate the resulting model using ROUGE metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b6ce61-1b18-412e-8ea9-b1aa4b6ea33d",
   "metadata": {},
   "source": [
    "## Install the pre-requisites \n",
    "\n",
    "Uncomment the following if these python packages have not been installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4da7e887-227f-4cce-983a-83ab3405de65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install transformers datasets accelerate sentencepiece scipy peft bitsandbytes ipywidgets nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39feac3-3c0f-4057-83e2-898a0329cbee",
   "metadata": {},
   "source": [
    "## Request access to Llama-2 weights\n",
    "\n",
    "You need to request for access to download the Llama 2 weights. You can either do so through this [link at Meta](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) or through your huggingface account at this (link)(https://huggingface.co/meta-llama/Llama-2-7b). Once your request is approved, you will receive an email from Meta with instruction to download the Llama 2 weights, or email from Hugging Face informing you access has been granted. \n",
    "\n",
    "If you download the weights from Meta directly, you need to run a conversion script to convert the weights to huggingface format for use with huggingface transformer library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4178ec51-ccf8-4d01-8224-e57c8b0683f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# TRANSFORM=`python -c \"import transformers;print('/'.join(transformers.__file__.split('/')[:-1])+'/models/llama/convert_llama_weights_to_hf.py')\"`\n",
    "# python ${TRANSFORM} --input_dir models --model_size 7B --output_dir models_hf/7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00149073-36b6-4317-818b-c8e0485cd8d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6effac582e48eb89ea4473ed45ec15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uncomment the following to login to HuggingFace to access the Llama model \n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4442511b-f066-470a-8ef2-f9d59c1a8733",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "We first import all the necessary python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9dece3-3170-4755-8e03-6ca272696baf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd54961-e4a8-418e-abe3-7797827269b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0e386f-b5ce-4ae6-9257-f84e8986cbea",
   "metadata": {},
   "source": [
    "## Some useful function\n",
    "\n",
    "Let's define a function to print out GPU utilization for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afac9589-3bf6-4de5-bc1d-e41476e42d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3aa994-e650-4b39-8bf9-5d888e2f3088",
   "metadata": {},
   "source": [
    "## Load the Pretrained Model and Tokenizer \n",
    "\n",
    "Load the pre-trained Llama 2 model and its tokenizer directly from HuggingFace. We will load the model in 8 bit quantization to save memory. For a more detailed understanding about how the model perform the matrix multiplication in 8-bit, see this [blog post](https://huggingface.co/blog/hf-bitsandbytes-integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93573bed-1060-481d-b75a-2ae0eac46e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39accbf7efe248cbabaa9d19faeb2275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008b5668ad054f69b46e1c04e1ec6244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id=\"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)\n",
    "model =LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afa2b8f5-bd17-4e09-a4c3-c87ae5c825e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"quantization_config\": {\n",
       "    \"bnb_4bit_compute_dtype\": \"float32\",\n",
       "    \"bnb_4bit_quant_type\": \"fp4\",\n",
       "    \"bnb_4bit_use_double_quant\": false,\n",
       "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "    \"llm_int8_has_fp16_weight\": false,\n",
       "    \"llm_int8_skip_modules\": null,\n",
       "    \"llm_int8_threshold\": 6.0,\n",
       "    \"load_in_4bit\": false,\n",
       "    \"load_in_8bit\": true,\n",
       "    \"quant_method\": \"bitsandbytes\"\n",
       "  },\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.34.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65428695-93cd-4478-81ea-29719cc64d5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 15313 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e48bfd9-9813-47e1-ba0e-ee08722f893f",
   "metadata": {},
   "source": [
    "The following shows the GPU memory consumption on an RTX GPU, with different model dtype.\n",
    "\n",
    "- load_in_8bit = 8224 MB\n",
    "- load_in_16bit = 13902 MB\n",
    "- load_in_32bit = 26830 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9574b5dd-8ba7-4fa9-944f-646bd49c62f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the dataset\n",
    "\n",
    "We are going to use the DialogSum Hugging Face dataset. It contains 10,000+ dialogues with the corresponding manually labeled summaries and topics. Note that the dataset is already split into train, validation and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c26a2ef-ed9c-4d03-acb5-2b9fc1224be7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fdbe4df-2e63-4f7f-864f-0fddbd7c370e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = dataset['train']\n",
    "dataset_test = dataset['test']\n",
    "dataset_val = dataset['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7da07-9192-4718-b10e-c47fa5634c02",
   "metadata": {},
   "source": [
    "Let's taka a look at one of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1ce8a86-3bc2-44d6-9cf1-3dcda2085ede",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'train_100',\n",
       " 'dialogue': \"#Person1#: I have a problem with my cable.\\n#Person2#: What about it?\\n#Person1#: My cable has been out for the past week or so.\\n#Person2#: The cable is down right now. I am very sorry.\\n#Person1#: When will it be working again?\\n#Person2#: It should be back on in the next couple of days.\\n#Person1#: Do I still have to pay for the cable?\\n#Person2#: We're going to give you a credit while the cable is down.\\n#Person1#: So, I don't have to pay for it?\\n#Person2#: No, not until your cable comes back on.\\n#Person1#: Okay, thanks for everything.\\n#Person2#: You're welcome, and I apologize for the inconvenience.\",\n",
       " 'summary': \"#Person1# has a problem with the cable. #Person2# promises it should work again and #Person1# doesn't have to pay while it's down.\",\n",
       " 'topic': 'cable'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8358e2e-e29d-4d5c-89f9-f78959ef5527",
   "metadata": {},
   "source": [
    "## Test the Model with Zero Shot Inferencing\n",
    "\n",
    "Test the model with the zero shot inferencing. You can see that the model struggles to summarize the dialogue compared to the baseline summary, and it is just repeating the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31def3d5-e863-4b34-8c2e-408a0cb8f0b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 23:59:44.940735: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-30 23:59:44.983556: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-30 23:59:45.554198: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize this dialog:\n",
      "#Person1#: I have a problem with my cable.\n",
      "#Person2#: What about it?\n",
      "#Person1#: My cable has been out for the past week or so.\n",
      "#Person2#: The cable is down right now. I am very sorry.\n",
      "#Person1#: When will it be working again?\n",
      "#Person2#: It should be back on in the next couple of days.\n",
      "#Person1#: Do I still have to pay for the cable?\n",
      "#Person2#: We're going to give you a credit while the cable is down.\n",
      "#Person1#: So, I don't have to pay for it?\n",
      "#Person2#: No, not until your cable comes back on.\n",
      "#Person1#: Okay, thanks for everything.\n",
      "#Person2#: You're welcome, and I apologize for the inconvenience.\n",
      "---\n",
      "Summary:\n",
      "#Person1#: I have a problem with my cable.\n",
      "#Person2#: What about it?\n",
      "#Person1#: My cable has been out for the past week or so.\n",
      "#Person2#: The cable is down right now. I am very sorry.\n",
      "#Person1#: When will it be working again?\n",
      "#Person2#: It should be back on in the next couple of days.\n",
      "#Person1#: Do I still have to pay for the cable?\n",
      "#Person2#: We're going to give you a credit while the cable is down.\n",
      "#Person1#: So, I don't have to pay for it?\n",
      "#Person2#: No, not until your cable comes back on.\n",
      "#Person1#: Okay, thanks for everything.\n",
      "#Person2#: You're welcome, and I apologize for the inconvenience.\n",
      "---\n",
      "Summary:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"\n",
    "Summarize this dialog:\n",
    "#Person1#: I have a problem with my cable.\n",
    "#Person2#: What about it?\n",
    "#Person1#: My cable has been out for the past week or so.\n",
    "#Person2#: The cable is down right now. I am very sorry.\n",
    "#Person1#: When will it be working again?\n",
    "#Person2#: It should be back on in the next couple of days.\n",
    "#Person1#: Do I still have to pay for the cable?\n",
    "#Person2#: We're going to give you a credit while the cable is down.\n",
    "#Person1#: So, I don't have to pay for it?\n",
    "#Person2#: No, not until your cable comes back on.\n",
    "#Person1#: Okay, thanks for everything.\n",
    "#Person2#: You're welcome, and I apologize for the inconvenience.\n",
    "---\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=200)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf285722-2ac5-4e60-8cc6-06c048e6d723",
   "metadata": {},
   "source": [
    "## Data Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4012814-8355-4d75-8d0e-e1efbd2ba8de",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Instruction prompt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47021de5-406b-4bc4-9819-063dfe800875",
   "metadata": {},
   "source": [
    "We need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM such as follows:\n",
    "\n",
    "```\n",
    "Summarize this dialog:\n",
    "\n",
    "#Person1#: This is Person1 part of the conversation.\n",
    "#Person2#: This is Person2 part of the conversation.\n",
    "---\n",
    "Summary: \n",
    "This is ground truth summary of the dialog.\n",
    "```\n",
    "\n",
    "We will create a prompt template and a function to apply the template to all the samples in the dataset. Note that we also append a eos token to the end of the sample. This is so that the fine-tuned model will learn to end the sentence at the appropriate time (e.g. end of the summary) instead of generating tokens infinitely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15fa67b5-4a13-4c74-9f7f-a4163f90ae95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b45ac375ee45ee82251190c56a5ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def apply_prompt_template(sample):\n",
    "    prompt = (\n",
    "        f\"Summarize this dialog:\\n{{dialog}}\\n---\\nSummary:\\n{{summary}}{{eos_token}}\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"text\": prompt.format(\n",
    "            dialog=sample[\"dialogue\"],\n",
    "            summary=sample[\"summary\"],\n",
    "            eos_token=tokenizer.eos_token,\n",
    "        )\n",
    "    }\n",
    "            \n",
    "dataset_train = dataset_train.map(apply_prompt_template, remove_columns=list(dataset_train.features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6b3ad5-e1ca-40ed-af2e-a042b4e14f2f",
   "metadata": {},
   "source": [
    "Let's look at one of the sample. We can see that the original sample has been converted to sample with a single 'text' field, and the text now confirms to the template we specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df57feef-dccd-4170-83c1-50e2f917a660",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Summarize this dialog:\\n#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\\n---\\nSummary:\\nMr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.</s>\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc3398-53ac-40a0-9c01-5cb5f02e0bd8",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "Apply the prompt template to the validation and test splits too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69930bd2-f847-433d-8144-c7a2fa732b15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1895c54dec08440c9337f13ebd70c19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89b1b82cb534dd687e571f24330753a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Complete your code here \n",
    "\n",
    "dataset_val = dataset_val.map(apply_prompt_template, remove_columns=list(dataset_val.features))\n",
    "dataset_test = dataset_test.map(apply_prompt_template, remove_columns=list(dataset_test.features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e5b558-0dfc-43de-968a-0865f7d35c4b",
   "metadata": {},
   "source": [
    "## Tokenization and Preparing the Input "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a283c3-0209-4a3a-a26c-875c2e0b9bf5",
   "metadata": {},
   "source": [
    "### Tokenization \n",
    "\n",
    "Before we can use the dataset for training, we first need to tokenize the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92077d4b-bceb-4b5a-81a4-74022d1ec96e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ec8a1e63954213899c86b04b68fac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "dataset_train_tokenized = dataset_train.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=dataset_train.features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2521c5e7-8976-473a-b7ce-e5bd1b454ab6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info:  Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 12460\n",
      "})\n",
      "Length of input_ids:  341\n",
      "Sample input: \n",
      " {'input_ids': [1, 6991, 3034, 675, 445, 7928, 29901, 13, 29937, 7435, 29896, 29937, 29901, 6324, 29892, 3237, 29889, 7075, 29889, 306, 29915, 29885, 15460, 10875, 11335, 29889, 3750, 526, 366, 1244, 9826, 29973, 13, 29937, 7435, 29906, 29937, 29901, 306, 1476, 372, 723, 367, 263, 1781, 2969, 304, 679, 263, 1423, 29899, 786, 29889, 13, 29937, 7435, 29896, 29937, 29901, 3869, 29892, 1532, 29892, 366, 7359, 29915, 29873, 750, 697, 363, 29871, 29945, 2440, 29889, 887, 881, 505, 697, 1432, 1629, 29889, 13, 29937, 7435, 29906, 29937, 29901, 306, 1073, 29889, 306, 4377, 408, 1472, 408, 727, 338, 3078, 2743, 29892, 2020, 748, 1074, 278, 11619, 29973, 13, 29937, 7435, 29896, 29937, 29901, 5674, 29892, 278, 1900, 982, 304, 4772, 10676, 4486, 2264, 267, 338, 304, 1284, 714, 1048, 963, 4688, 29889, 1105, 1018, 304, 2041, 472, 3203, 2748, 263, 1629, 363, 596, 1914, 1781, 29889, 13, 29937, 7435, 29906, 29937, 29901, 3674, 29889, 13, 29937, 7435, 29896, 29937, 29901, 2803, 592, 1074, 1244, 29889, 3575, 5076, 322, 22827, 1106, 2691, 29889, 11190, 263, 6483, 16172, 29892, 3113, 29889, 1938, 366, 25158, 29892, 3237, 29889, 7075, 29973, 13, 29937, 7435, 29906, 29937, 29901, 3869, 29889, 13, 29937, 7435, 29896, 29937, 29901, 4116, 17223, 338, 278, 8236, 4556, 310, 13030, 23900, 322, 5192, 17135, 29892, 366, 1073, 29889, 887, 2289, 881, 23283, 29889, 13, 29937, 7435, 29906, 29937, 29901, 306, 29915, 345, 1898, 21006, 310, 3064, 29892, 541, 306, 925, 508, 29915, 29873, 2833, 304, 24817, 278, 4760, 29889, 13, 29937, 7435, 29896, 29937, 29901, 5674, 29892, 591, 505, 4413, 322, 777, 13589, 800, 393, 1795, 1371, 29889, 306, 29915, 645, 2367, 366, 901, 2472, 1434, 366, 5967, 29889, 13, 29937, 7435, 29906, 29937, 29901, 3674, 29892, 3969, 11619, 29889, 13, 5634, 13, 26289, 29901, 13, 20335, 29889, 7075, 29915, 29879, 2805, 263, 1423, 29899, 786, 29892, 322, 15460, 10875, 11335, 25228, 267, 1075, 304, 505, 697, 1432, 1629, 29889, 10875, 11335, 29915, 645, 2367, 777, 2472, 1048, 1009, 4413, 322, 13589, 800, 304, 1371, 3237, 29889, 7075, 23283, 1560, 17223, 29889, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset info: \", dataset_train_tokenized)\n",
    "print(\"Length of input_ids: \", len(dataset_train_tokenized['input_ids'][0]))\n",
    "print(\"Sample input: \\n\", dataset_train_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967dee0e-3fa3-4388-a32e-6a3cf118bccb",
   "metadata": {},
   "source": [
    "We can see that after tokenization, we now have input_ids (which contains the id corresponding to a token (subword), and the attention mask, the attention mask tells the model which token to ignore (e.g. padding). We also shown the input_ids length of the first sample, which in this case is 341 (token ids)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4942aa44-4850-45f1-9a00-932a9ba3d9a1",
   "metadata": {},
   "source": [
    "We will do the same tokenization on our validation dataset and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "617a9913-45d2-4bde-917a-e39df9cb291c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2870f08c3d1345b6a1ad52c29f656a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc44415a0f254580bf91ef0f27402bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_val_tokenized = dataset_val.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=dataset_val.features,\n",
    ")\n",
    "\n",
    "dataset_test_tokenized = dataset_test.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=dataset_test.features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a143a35f-901b-4543-bd1b-2b31e62f3fc4",
   "metadata": {},
   "source": [
    "Now let's prepare the input data to the moodel. As you can see above, typically the length of the token ids (input_ids) are few hundred tokens long. However, Llama model typically have 2048 or 4096 context window. To use the data efficiently, we use a technique called packing: instead of having one text per sample in the batch and then padding to either the longest text or the maximal context of the model, we concatenate a lot of texts with a EOS token in between and cut chunks of the context size to fill the batch without any padding.\n",
    "\n",
    "<img src=\"packing.png\" width=\"500\" height=\"600\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb53e5ba-f34a-46bc-8f6b-0a0414a97bd3",
   "metadata": {},
   "source": [
    "The code below help us find the maximum context window of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "310776bb-8f44-4199-925a-031d271c53bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max context lenth: 4096\n",
      "Context length:  4096\n"
     ]
    }
   ],
   "source": [
    "def get_max_context_length(model):\n",
    "    \n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    \n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max context lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max context length: {max_length}\")\n",
    "        \n",
    "    return max_length\n",
    "\n",
    "context_length = get_max_context_length(model)\n",
    "print('Context length: ', context_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75a8059-64c8-41dd-a083-4171dcb4cd7c",
   "metadata": {},
   "source": [
    "The following functions concatenate a batch of samples, and then divide the concatenated sample into chunks of context size.  Also we also need to create 'labels' in the input dataset, which tells the model what is the token to be predicted.  Shifting the inputs and labels to align them happens inside the model, so our labels are just the exact copy of the input_ids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd6d2b52-fdec-4c36-9be4-c9922dcb62f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context_length = 2048\n",
    "\n",
    "def group_texts(examples):\n",
    "    \n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= context_length:\n",
    "        total_length = (total_length // context_length) * context_length\n",
    "    # Split by chunks of context length.\n",
    "    result = {\n",
    "        k: [t[i : i + context_length] for i in range(0, total_length, context_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de68071b-2cc9-4931-abbc-cb495c83f538",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25f22f78b8543148c6ff043a52e834e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_train_final = dataset_train_tokenized.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9591f1bb-2071-42fb-b50c-f9c6d3f7abca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a765f3c4cc604410b17c168adee87b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80aefa3b681247dda069178d1f573d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_val_final = dataset_val_tokenized.map(group_texts, batched=True, num_proc=4)\n",
    "dataset_test_final = dataset_test_tokenized.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceafcf63-dbb1-4c60-8a83-ef3417b8ac19",
   "metadata": {},
   "source": [
    "Now let's examine the dataset_train_final and we can see that all the samples are of lenghth equal to the specified context window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b28c5b2-4137-4dd6-b071-5894627cc917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1688\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd8ea214-14ec-464d-bb5c-b72bbb39091d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7623184b-18b0-4dfd-84be-2f2750872c77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "for sample in dataset_train_final['input_ids'][:5]: \n",
    "    print(len(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "376d35d2-d3e2-4aed-9bb5-08606cddf7ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd17bc21-0252-4436-a462-924e22934286",
   "metadata": {},
   "source": [
    "## Setup the PEFT/LoRA model for Fine-Tuning\n",
    "\n",
    "You need to set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LoRA, you are freezing the underlying LLM and only training the adapter. Have a look at the LoRA configuration below. Note the rank (r) hyper-parameter, which defines the rank/dimension of the adapter to be trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79bbcc28-be35-4e17-a6ec-d5e5d7695c80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markk/miniconda3/envs/diffuser_env/lib/python3.11/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "def create_peft_config(model):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "        prepare_model_for_int8_training,\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    )\n",
    "\n",
    "    # prepare int-8 model for training\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "\n",
    "# create peft config\n",
    "model, lora_config = create_peft_config(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ec086ae-9ad9-4599-b14e-d1d1fbaca197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import default_data_collator, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17ee0f84-cb95-4990-927a-e87384bfc2f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = \"tmp/mkk_llama2-output\"\n",
    "\n",
    "config = {\n",
    "    'lora_config': lora_config,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_train_epochs': 1,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'per_device_train_batch_size': 2,\n",
    "    'gradient_checkpointing': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb09a595-525a-410f-9660-a071aba14846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    bf16=False,  # Use BF16 if available\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    max_steps=300,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    ")\n",
    "\n",
    "    # Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train_final,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "699b9051-01d7-4ba5-82ad-13bf2bac2bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/markk/miniconda3/envs/diffuser_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/markk/miniconda3/envs/diffuser_env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 1:48:36, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.520200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.368200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.349200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.357100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.317900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.314800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.299100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.316800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.291400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.288300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.281200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.308900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.324400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.289400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.303800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.292800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.331400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.278100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.303900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.320900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.290100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.318500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.290800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.294600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.285600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.302500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.277700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=1.3186125469207763, metrics={'train_runtime': 6538.1563, 'train_samples_per_second': 0.184, 'train_steps_per_second': 0.046, 'total_flos': 9.7491093553152e+16, 'train_loss': 1.3186125469207763, 'epoch': 0.71})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdcc4361-f0c9-4c30-871f-19746b0eb504",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 17750 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2517d3ee-6e4b-4c28-ad03-50dd517827ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from peft import (\n",
    "#     get_peft_model,\n",
    "#     LoraConfig, \n",
    "#     TaskType, \n",
    "#     prepare_model_for_int8_training\n",
    "# )\n",
    "\n",
    "# lora_config = LoraConfig(\n",
    "#     r=8, # Rank\n",
    "#     lora_alpha=32,\n",
    "#     inference_mode=False,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"],\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=TaskType.CAUSAL_LM\n",
    "# )\n",
    "\n",
    "# model.add_adapter(lora_config)\n",
    "# # model = prepare_model_for_int8_training(model)\n",
    "# # model = get_peft_model(model, lora_config)\n",
    "# # model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1db85ec9-15db-4b88-8c50-2fa1b54d5ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 4194304\n",
      "all model parameters: 6742609920\n",
      "percentage of trainable model parameters: 0.06%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8bc3ead9-0890-4526-90d1-1e98909430dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9942a38-1911-4009-af11-ef73849602c5",
   "metadata": {},
   "source": [
    "If you look at the trainable prarameters, there are only about 4 million parameters, comparaed to about 6.7 billion parameters of the entire model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc5b40-95a1-4e06-aa3d-67b2b09a6265",
   "metadata": {},
   "source": [
    "## Define the Trainer and Training Arguments \n",
    "\n",
    "We can now define training arguments and create Trainer instance. If you are using Ampere GPU (e.g. NVIDIA A10), then you can set bf16 to True to use bfloat16 for mixed precision computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10853496-3414-4176-875c-788e5707b8e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import default_data_collator, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "output_dir = \"tmp/llama-output\"\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    # auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    bf16=False,  # Use BF16 if available\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy ='steps',\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    num_train_epochs=1,\n",
    "    load_best_model_at_end=True,\n",
    "    # max_steps=300\n",
    ")\n",
    "\n",
    "    # Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train_final,\n",
    "    # eval_dataset=dataset_val_final,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start trainingwe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3f26e1a-ed52-422c-a3c3-f3495208c87e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='422' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 11/422 03:24 < 2:35:21, 0.04 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Trainer: evaluation requires an eval_dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/diffuser_env/lib/python3.11/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1592\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1593\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1594\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1596\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/diffuser_env/lib/python3.11/site-packages/transformers/trainer.py:1984\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1981\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1984\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/miniconda3/envs/diffuser_env/lib/python3.11/site-packages/transformers/trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2326\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mupdate(dataset_metrics)\n\u001b[1;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2328\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys_for_eval)\n\u001b[1;32m   2329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2331\u001b[0m \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffuser_env/lib/python3.11/site-packages/transformers/trainer.py:3062\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3059\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m   3060\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m-> 3062\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_eval_dataloader(eval_dataset)\n\u001b[1;32m   3063\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3065\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n",
      "File \u001b[0;32m~/miniconda3/envs/diffuser_env/lib/python3.11/site-packages/transformers/trainer.py:888\u001b[0m, in \u001b[0;36mTrainer.get_eval_dataloader\u001b[0;34m(self, eval_dataset)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;124;03mReturns the evaluation [`~torch.utils.data.DataLoader`].\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;124;03m        by the `model.forward()` method are automatically removed. It must implement `__len__`.\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer: evaluation requires an eval_dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    889\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m eval_dataset \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset\n\u001b[1;32m    890\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_collator\n",
      "\u001b[0;31mValueError\u001b[0m: Trainer: evaluation requires an eval_dataset."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93524441-36a1-4f7e-8307-b855e68c5c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset=dataset_val_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825728df-3d1e-4beb-876e-8de547077cfa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "035b3726-b8e4-4c1f-834f-12b33204bf97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d0595e2-555a-469d-9381-f2fb3c165873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "save_dir = 'lora_model_output'\n",
    "model.save_pretrained(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bb4856bd-bfb3-48c2-817a-b9ccb271c449",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1d140ca6874ba696269964275ff29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch \n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-hf'\n",
    "save_dir = 'lora_model_output'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "peft_model = AutoModelForCausalLM.from_pretrained(save_dir, device_map='cuda:0', load_in_8bit=True, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b56996a5-c0ab-4bfb-8c84-b267944702ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear8bitLt(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4e3e500b-e181-46db-af20-4b51eb4a4cba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "num_of_gpus = torch.cuda.device_count()\n",
    "print(num_of_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6645dc1-c797-4044-84c5-be6955d66252",
   "metadata": {},
   "source": [
    "### Test the Model \n",
    "\n",
    "Now let's test our fine-tuned model on the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c96ae5df-3f35-459e-be1b-0c6bb9ed487a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize this dialog:\n",
      "#Person1#: Hello, how are you doing today?\n",
      "#Person2#: I ' Ve been having trouble breathing lately.\n",
      "#Person1#: Have you had any type of cold lately?\n",
      "#Person2#: No, I haven ' t had a cold. I just have a heavy feeling in my chest when I try to breathe.\n",
      "#Person1#: Do you have any allergies that you know of?\n",
      "#Person2#: No, I don ' t have any allergies that I know of.\n",
      "#Person1#: Does this happen all the time or mostly when you are active?\n",
      "#Person2#: It happens a lot when I work out.\n",
      "#Person1#: I am going to send you to a pulmonary specialist who can run tests on you for asthma.\n",
      "#Person2#: Thank you for your help, doctor.\n",
      "---\n",
      "Summary:\n",
      "#Person2# tells #Person1# #Person2# has a heavy feeling in #Person2#'s chest when trying to breathe. #Person1# will send #Person2# to a pulmonary specialist.\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"\n",
    "Summarize this dialog:\n",
    "#Person1#: Hello, how are you doing today?\n",
    "#Person2#: I ' Ve been having trouble breathing lately.\n",
    "#Person1#: Have you had any type of cold lately?\n",
    "#Person2#: No, I haven ' t had a cold. I just have a heavy feeling in my chest when I try to breathe.\n",
    "#Person1#: Do you have any allergies that you know of?\n",
    "#Person2#: No, I don ' t have any allergies that I know of.\n",
    "#Person1#: Does this happen all the time or mostly when you are active?\n",
    "#Person2#: It happens a lot when I work out.\n",
    "#Person1#: I am going to send you to a pulmonary specialist who can run tests on you for asthma.\n",
    "#Person2#: Thank you for your help, doctor.\n",
    "---\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# eval_prompt = \"\"\"\n",
    "# Summarize this dialog:\n",
    "# A: Hi Tom, are you busy tomorrowâ€™s afternoon?\n",
    "# B: Iâ€™m pretty sure I am. Whatâ€™s up?\n",
    "# A: Can you go with me to the animal shelter?.\n",
    "# B: What do you want to do?\n",
    "# A: I want to get a puppy for my son.\n",
    "# B: That will make him so happy.\n",
    "# A: Yeah, weâ€™ve discussed it many times. I think heâ€™s ready now.\n",
    "# B: Thatâ€™s good. Raising a dog is a tough issue. Like having a baby ;-) \n",
    "# A: I'll get him one of those little dogs.\n",
    "# B: One that won't grow up too big;-)\n",
    "# A: And eat too much;-))\n",
    "# B: Do you know which one he would like?\n",
    "# A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
    "# B: I bet you had to drag him away.\n",
    "# A: He wanted to take it home right away ;-).\n",
    "# B: I wonder what he'll name it.\n",
    "# A: He said heâ€™d name it after his dead hamster â€“ Lemmy  - he's  a great Motorhead fan :-)))\n",
    "# ---\n",
    "# Summary:\n",
    "# \"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d88d004-2b52-49c0-a512-5518bd991ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "baseline_human_summary = dataset['test'][index]['summary']\n",
    "\n",
    "eval_prompt = f\"\"\"\n",
    "Summarize this dialog:\n",
    "{dialogue}\n",
    "---\n",
    "Summary: \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1498e82d-ecb6-47f7-ad98-2381bed86417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_summary(text): \n",
    "    parts = re.split(r'Summary:', text)\n",
    "    summary = parts[1].strip()\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "05b7bd01-34c7-44d5-bc7b-5ffd36e350ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"#Person1#: Hello, how are you doing today?\\n#Person2#: I ' Ve been having trouble breathing lately.\\n#Person1#: Have you had any type of cold lately?\\n#Person2#: No, I haven ' t had a cold. I just have a heavy feeling in my chest when I try to breathe.\\n#Person1#: Do you have any allergies that you know of?\\n#Person2#: No, I don ' t have any allergies that I know of.\\n#Person1#: Does this happen all the time or mostly when you are active?\\n#Person2#: It happens a lot when I work out.\\n#Person1#: I am going to send you to a pulmonary specialist who can run tests on you for asthma.\\n#Person2#: Thank you for your help, doctor.\",\n",
       " \"#Person1#: Hey Jimmy. Let's go workout later today.\\n#Person2#: Sure. What time do you want to go?\\n#Person1#: How about at 3:30?\\n#Person2#: That sounds good. Today we work on Legs and forearm.\\n#Person1#: Hey. I just played basketball earlier, so my legs are a little sore. Let's work out on arms and stomach today.\\n#Person2#: I'm on a weekly schedule. You're messing everything up.\\n#Person1#: C'mon. We're only switching two days. You can do legs on Friday.\\n#Person2#: Aright. I'll meet you at the gym at 3:30 then.\",\n",
       " \"#Person1#: I need to stop eating such unhealthy foods.\\n#Person2#: I know what you mean. I've started eating better myself.\\n#Person1#: What foods do you eat now?\\n#Person2#: I tend to stick to fruits, vegetables, and chicken.\\n#Person1#: Those are the only things you eat?\\n#Person2#: That's basically what I eat.\\n#Person1#: Why aren't you eating anything else?\\n#Person2#: Well, fruits and vegetables are very healthy.\\n#Person1#: And the chicken?\\n#Person2#: It's really healthy to eat when you bake it.\\n#Person1#: I guess that does sound a lot healthier.\",\n",
       " \"#Person1#: Do you believe in UFOs?\\n#Person2#: Of course, they are out there.\\n#Person1#: But I never saw them.\\n#Person2#: Are you stupid? They are called UFOs, so not everybody can see them.\\n#Person1#: You mean that you can them.\\n#Person2#: That's right. I can see them in my dreams.\\n#Person1#: They come to the earth?\\n#Person2#: No. Their task is to send the aliens here from the outer space.\\n#Person1#: Aliens from the outer space? Do you talk to them? What do they look like?\\n#Person2#: OK, OK, one by one, please! They look like robots, but they can speak. Their mission is to make friends with human beings.\\n#Person1#: That means that you talk to them? In which language?\\n#Person2#: Of course in English, they learn English on Mars too.\\n#Person1#: Wow. Sounds fantastic!\",\n",
       " \"#Person1#: Did you go to school today?\\n#Person2#: Of course. Did you?\\n#Person1#: I didn't want to, so I didn't.\\n#Person2#: That's sad, but have you gone to the movies recently?\\n#Person1#: That's a switch.\\n#Person2#: I'm serious, have you?\\n#Person1#: No, I haven't. Why?\\n#Person2#: I really want to go to the movies this weekend.\\n#Person1#: So go then.\\n#Person2#: I really don't want to go by myself.\\n#Person1#: Well anyway, do you plan on going to school tomorrow?\\n#Person2#: No, I think I'm going to go to the movies.\",\n",
       " \"#Person1#: Honey, I think you should quit smoking.\\n#Person2#: Why? You said I was hot when smoking.\\n#Person1#: But I want you to be fit.\\n#Person2#: Smoking is killing. I know.\\n#Person1#: Check out this article. It says smoking can lead to lung cancer.\\n#Person2#: I don't believe it.\\n#Person1#: But you know that smoking does harm to health, right?\\n#Person2#: Of course I know it, but you know it's hard to quit smoking. . .\\n#Person1#: Stop beating around the bush. Will you quit or not?\\n#Person2#: Yes, ma'am. Whatever you say.\",\n",
       " \"#Person1#: Excuse me, Mr. White? I just need you to sign these before I leave.\\n#Person2#: Sure, Sherry. Sorry to have kept you waiting. If you hadn't told me, I probably would have just forgotten all about them.\\n#Person1#: That's my job, sir. Just one more signature here, please.\\n#Person2#: There you are.\",\n",
       " \"#Person1#: Hey, Karen. Look like you got some sun this weekend.\\n#Person2#: Yeah? I guess so. I spent the weekend at beach.\\n#Person1#: That's great. Where did you stay?\\n#Person2#: Some friends of my parents live out there, and they invited me there.\\n#Person1#: So, what did you do out there? I mean besides bask in the sun, obviously.\\n#Person2#: I jogged up and down the beach and played volleyball. You know I never realized how hard it is to run on sand. I couldn't get through a whole game before I had to sit down.\\n#Person1#: Not to mention cooler. Did you go swimming?\\n#Person2#: I wanted to, but the water is too cold, and I just wetted in up to my knees.\\n#Person1#: It all sounds so relaxing. I wish I could get away to the beach like that.\\n#Person2#: It looks like you could use it. Don't tell me you spent the weekend in the library again.\",\n",
       " \"#Person1#: How do you usually spend your leisure time? I mean, do you have any special interests out of your job?\\n#Person2#: Of course. You see, almost everyone has some kind of hobby\\n#Person1#: Yeah, you're quite right and what's your hobby?\\n#Person2#: I like taking photos out of door.\\n#Person1#: Oh, photography, It's really a good hobby.\\n#Person2#: Yes, I usually develop and print all my own photos.\\n#Person1#: You yourself have a photo studio?\\n#Person2#: Yes, simple as it is. It does work.\",\n",
       " \"#Person1#: have you ever seen Bill Gate's home on the internet?\\n#Person2#: no. what's it like?\\n#Person1#: it's got its own library, theatre, swimming pool, and a guest house. The house itself has about ten different rooms that are all hooked up to computers so you can get things done in each room through.\\n#Person2#: would you want to live there?\\n#Person1#: I think his house is fantastic, but I wouldn't want to live there. You would have to hire one or two people to clean all the rooms in the house, plus a few people to take care of the gardens.\\n#Person2#: what's your dream home like then?\\n#Person1#: my dream home is actually just a small cottage in a quite village in England.\\n#Person2#: would you want to buy an old cottage or build a new one yourself?\\n#Person1#: old homes are great because they've got character. I think that's important.\\n#Person2#: it that why you wear second-hand cloths as well? Because they've got character?\\n#Person1#: no, that's just because I don't have enough money to buy new cloths all the time!\\n#Person2#: I see. If you lived in an old house, would it be decorated in a modern way?\\n#Person1#: no, I'd definitely try to restore it to its original state. I love to imagine what it'd be like to live in another time in history and living in a house decorated like it would have been 200 years a\"]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation']['dialogue'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "91dd3ea1-6cdc-47df-b62f-416a9b02bcd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['test']['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "32a0602d-47a3-4e1f-aa3b-80631d9398b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dialogues = dataset['test']['dialogue'][:20]\n",
    "human_baseline_summaries = dataset['test']['summary'][:20]\n",
    "peft_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    eval_prompt = f\"\"\"\n",
    "Summarize this dialog:\n",
    "{dialogue}\n",
    "---\n",
    "Summary: \n",
    "\"\"\"\n",
    "    # print(eval_prompt)\n",
    "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        peft_model_output = tokenizer.decode(peft_model.generate(**model_input)[0], skip_special_tokens=True)\n",
    "        # print(peft_model_output)\n",
    "    summary = get_summary(peft_model_output)\n",
    "    peft_model_summaries.append(summary)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd56caaf-47bb-454d-8046-a20a54a93eca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.',\n",
       " 'In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.',\n",
       " 'Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.',\n",
       " '#Person2# arrives late because of traffic jam. #Person1# persuades #Person2# to use public transportations to keep healthy and to protect the environment.',\n",
       " \"#Person2# decides to follow #Person1#'s suggestions on quitting driving to work and will try to use public transportations.\",\n",
       " '#Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.',\n",
       " '#Person1# tells Kate that Masha and Hero get divorced. Kate is surprised because she thought they are perfect couple.',\n",
       " '#Person1# tells Kate that Masha and Hero are getting a peaceful divorce. Kate feels surprised and asks about their kids.',\n",
       " '#Person1# and Kate talk about the divorce between Masha and Hero. Kate feels surprised because she thought they are well matched',\n",
       " '#Person1# and Brian are at the birthday party of Brian. Brian thinks #Person1# looks great and is popular.',\n",
       " \"#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\",\n",
       " \"#Person1# has a dance with Brian at Brian's birthday party. Brian thinks #Person1# looks great and is popular.\",\n",
       " \"#Person1# is surprised at the Olympic Stadium'volume, capacity and interior setting to #Person1#.\",\n",
       " '#Person2# shows #Person1# around the constructing Olympic stadium and introduces the stadium.',\n",
       " \"#Person2# introduces the Olympic Stadium's finishing time, capacity and interior setting to #Person1#.\",\n",
       " '#Person1# wants to create a company and is going to write a business plan. #Person2# gives #Person1# suggestions on how to summarise business ideas, describe the service, differ from competitors and attract investment in a good business plan. #Person1# decides to stick to the old job.',\n",
       " '#Person1# abandons the idea of creating a company after #Person2# explains what a business plan includes. #Person2# specifies that a well-written business plan includes an executive summary, goods and service description, strategy and implementation, and financial analysis.',\n",
       " \"#Person1# wants to start #Person1#'s own business, but #Person2# warns #Person1# of the hassle. #Person2# tells #Person1# what is needed in a business plan and #Person1# decides to stick to the old job for now.\",\n",
       " \"#Person2# feels itchy. #Person1# doubts it is chicken pox and asks #Person2# to get away. #Person2# doesn't believe it.\",\n",
       " '#Person1# suspects that #Person2# has chicken pox and wants to keep away from #Person2#. #Person2# thinks #Person1# is exaggerating.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_baseline_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1f0ab766-58d8-478b-8071-67337d303022",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"#Person1# asks #Person2# to take a dictation for him and #Person1#'s memo will be sent out to all employees.\",\n",
       " '#Person1# needs Ms. Dawson to take a dictation for #Person1#. #Person1# tells her the new policy about internal and external communications.',\n",
       " '#Person1# needs #Person2# to take a dictation for him. #Person1# wants to restrict all office communications to email correspondence and official memos. #Person2# asks about the new policy. #Person1# tells #Person2# the new policy applies to internal and external communications.',\n",
       " '#Person2# got stuck in traffic again. #Person1# suggests #Person2# take public transportation and suggests #Person2# try biking to work.',\n",
       " \"#Person2#'s stuck in traffic again. #Person1# suggests #Person2# try to find a different route to get home and take public transport system to work.\",\n",
       " '#Person2# got stuck in traffic again and #Person1# advises #Person2# to take public transport system to work.',\n",
       " '#Person1# tells Kate that Masha and Hero are getting divorced. #Person1# explains the reason and the process of the divorce.',\n",
       " 'Kate tells #Person2# that Masha and Hero are getting divorced. They were well matched and there is no quarrelling about who gets the house and stock.',\n",
       " \"Kate and #Person1# are talking about Masha and Hero's divorce. Kate is surprised and thinks it's unexpected.\",\n",
       " \"Brian is surprised that #Person1# remembers his birthday. #Person1# invites Brian to dance and asks him to have a drink to celebrate Brian's birthday.\",\n",
       " \"#Person1# and Brian have a dance together at Brian's birthday party.\",\n",
       " 'Brian is celebrating his birthday. #Person1# brings Brian a gift and they dance together.',\n",
       " '#Person1# and #Person2# are visiting the Olympic stadium.',\n",
       " '#Person1# and #Person2# are in the Olympic stadium.',\n",
       " '#Person1# and #Person2# are in the Olympic stadium.',\n",
       " \"#Person1# is fed up with the company and decides to create his own company. #Person2# tells #Person1# that it's not easy to write up a business plan and it takes a lot of time and effort.\",\n",
       " \"#Person1# is going to create a company and #Person2# tells #Person1# what a business plan needs to include. #Person1# thinks it's too hard.\",\n",
       " \"#Person1# is done working for a company that is taking #Person1# nowhere. #Person1# wants to create #Person1#'s own company. #Person2# explains to #Person1# the business plan and the difficulties.\",\n",
       " \"#Person2#'s itchy and #Person1# thinks #Person2# has chicken pox. #Person1#'s scared and tells #Person2# to go to the doctor.\",\n",
       " '#Person2# feels itchy and thinks #Person2# may be coming down with something. #Person1# thinks #Person2# has chicken pox and #Person1# is afraid of being infected.']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2a42d98a-ae8b-4453-88b2-94ec290e9529",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL:\n",
      "{'rouge1': 0.4558374199488039, 'rouge2': 0.14392726266439082, 'rougeL': 0.33775322870667207, 'rougeLsum': 0.3369952167804502}\n"
     ]
    }
   ],
   "source": [
    "import evaluate \n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "41045d0e-e5ec-45b0-8cc1-2c9294a93a18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b830580a-ac0d-439e-a06b-bdc38f980f41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total inference time: 3.70 ms\n",
      "Number of tokens generated: 256\n",
      "Time per token: 0.12 ms/token\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'average' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of tokens generated: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m256\u001b[39m))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime per token: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m ms/token\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time_per_token))\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens per second: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m token/s\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(average))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'average' is not defined"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "tokens_per_second_list = [] \n",
    "\n",
    "for i in range(50):\n",
    "    start = time.time()\n",
    "    output = pipeline(eval_prompt, max_new_tokens=30, temperature=1, top_k=1, top_p=0.90)\n",
    "\n",
    "    delay = time.time()\n",
    "    total_time = (delay - start)\n",
    "    time_per_token = total_time / 30\n",
    "\n",
    "    # Calculate tokens per second\n",
    "    tokens_per_second = 30 / total_time\n",
    "    tokens_per_second_list.append(tokens_per_second)\n",
    "\n",
    "\n",
    "averare = sum(tokens_per_second_list) / len(tokens_per_second_list)\n",
    "# Print the results\n",
    "print(\"Total inference time: {:.2f} ms\".format(total_time))\n",
    "print(\"Number of tokens generated: {}\".format(256))\n",
    "print(\"Time per token: {:.2f} ms/token\".format(time_per_token))\n",
    "print(\"Tokens per second: {:.2f} token/s\".format(average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "329a4431-0c0e-4e20-9658-eba323050a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.99356170500567"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f9679046-37ec-4ddc-90fe-e5750d5b91d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"\\nSummarize this dialog:\\n#Person1#: What's wrong with you? Why are you scratching so much?\\n#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\\n#Person1#: Let me have a look. Whoa! Get away from me!\\n#Person2#: What's wrong?\\n#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\\n#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\\n#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\\n#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.\\n---\\nSummary: \\n#Person2# feels itchy and thinks it may be coming down with something. #Person1# thinks #Person2# has chicken po\"}]\n"
     ]
    }
   ],
   "source": [
    "output = pipeline(eval_prompt, max_new_tokens=30, temperature=1, top_k=1, top_p=0.90)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc71d9-c0a8-4010-a09e-ff4e2ec3e112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
