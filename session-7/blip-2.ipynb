{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107-2024S2/blob/main/session-7/blip-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfYE8mmDUIGg"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install matplotlib transformers datasets accelerate sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsLAEG2a5-5r"
      },
      "source": [
        "## BLIP-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnNrafWu6BSJ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load processor and main model\n",
        "blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Send the model to GPU to speed up inference\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI_UWlR8c_Ey"
      },
      "source": [
        "### Preprocessing Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6YeV_TEQFAA"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from urllib.request import urlopen\n",
        "\n",
        "# Load image of a supercar\n",
        "car_path = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/car.png\"\n",
        "image = Image.open(urlopen(car_path)).convert(\"RGB\")\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flZeiDFmQIIg"
      },
      "outputs": [],
      "source": [
        "# Preprocess the image\n",
        "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "inputs[\"pixel_values\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lHJ7LiKUhWf"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Convert to numpy and go from (1, 3, 224, 224) to (224, 224, 3) in shape\n",
        "image_inputs = inputs[\"pixel_values\"][0].detach().cpu().numpy()\n",
        "image_inputs = np.einsum('ijk->kji', image_inputs)\n",
        "image_inputs = np.einsum('ijk->jik', image_inputs)\n",
        "\n",
        "# Scale image inputs to 0-255 to represent RGB values\n",
        "scaler = MinMaxScaler(feature_range=(0, 255))\n",
        "image_inputs = scaler.fit_transform(image_inputs.reshape(-1, image_inputs.shape[-1])).reshape(image_inputs.shape)\n",
        "image_inputs = np.array(image_inputs, dtype=np.uint8)\n",
        "\n",
        "# Convert numpy array to Image\n",
        "Image.fromarray(image_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDoBrN56dBTF"
      },
      "source": [
        "### Preprocessing Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHJkauW9dFhB"
      },
      "outputs": [],
      "source": [
        "blip_processor.tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21Zh5Rx8QLAy"
      },
      "outputs": [],
      "source": [
        "# Preprocess the text\n",
        "text = \"Her vocalization was remarkably melodic\"\n",
        "token_ids = blip_processor(image, text=text, return_tensors=\"pt\")\n",
        "token_ids = token_ids.to(device, torch.float16)[\"input_ids\"][0]\n",
        "\n",
        "# Convert input ids back to tokens\n",
        "tokens = blip_processor.tokenizer.convert_ids_to_tokens(token_ids)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFnL_Yt3elPX"
      },
      "outputs": [],
      "source": [
        "# Replace the space token with an underscore\n",
        "tokens = [token.replace(\"Ä \", \"_\") for token in tokens]\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDx5elnHeHnT"
      },
      "source": [
        "### Use Case 1: Image Captioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFJ9-l8c8u3i"
      },
      "outputs": [],
      "source": [
        "# Load an AI-generated image of a supercar\n",
        "image = Image.open(urlopen(car_path)).convert(\"RGB\")\n",
        "\n",
        "# Convert an image into inputs and preprocess it\n",
        "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHIoTZa6eEaR"
      },
      "outputs": [],
      "source": [
        "# Generate image ids to be passed to the decoder (LLM)\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
        "\n",
        "# Generate text from the image ids\n",
        "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "generated_text = generated_text[0].strip()\n",
        "generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbGnyiYhlfQi"
      },
      "outputs": [],
      "source": [
        "url = \"https://upload.wikimedia.org/wikipedia/commons/7/70/Rorschach_blot_01.jpg\"\n",
        "image = Image.open(urlopen(url)).convert(\"RGB\")\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bIxPEySrJW0"
      },
      "outputs": [],
      "source": [
        "# Load rorschach image\n",
        "url = \"https://upload.wikimedia.org/wikipedia/commons/7/70/Rorschach_blot_01.jpg\"\n",
        "image = Image.open(urlopen(url)).convert(\"RGB\")\n",
        "\n",
        "# Generate caption\n",
        "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
        "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "generated_text = generated_text[0].strip()\n",
        "generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjHfQxWGkVYF"
      },
      "source": [
        "### Use Case 2: Visual Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "269NtauNFIze"
      },
      "outputs": [],
      "source": [
        "# Load an AI-generated image of a supercar\n",
        "image = Image.open(urlopen(car_path)).convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NsMF_hMCIIj"
      },
      "outputs": [],
      "source": [
        "# Visual Question Answering\n",
        "prompt = \"Question: Write down what you see in this picture. Answer:\"\n",
        "\n",
        "# Process both the image and the prompt\n",
        "inputs = blip_processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "# Generate text\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=30)\n",
        "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "generated_text = generated_text[0].strip()\n",
        "generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtpMIgUaScBD"
      },
      "outputs": [],
      "source": [
        "# Chat-like prompting\n",
        "prompt = \"Question: Write down what you see in this picture. Answer: A sports car driving on the road at sunset. Question: What would it cost me to drive that car? Answer:\"\n",
        "\n",
        "# Generate output\n",
        "inputs = blip_processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=30)\n",
        "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "generated_text = generated_text[0].strip()\n",
        "generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtvYbcF-H_t6"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "def text_eventhandler(*args):\n",
        "  question = args[0][\"new\"]\n",
        "  if question:\n",
        "    args[0][\"owner\"].value = \"\"\n",
        "\n",
        "    # Create prompt\n",
        "    if not memory:\n",
        "      prompt = \" Question: \" + question + \" Answer:\"\n",
        "    else:\n",
        "      template = \"Question: {} Answer: {}.\"\n",
        "      prompt = \" \".join(\n",
        "          [\n",
        "              template.format(memory[i][0], memory[i][1])\n",
        "              for i in range(len(memory))\n",
        "          ]\n",
        "      ) + \" Question: \" + question + \" Answer:\"\n",
        "\n",
        "    # Generate text\n",
        "    inputs = blip_processor(image, text=prompt, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(device, torch.float16)\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=100)\n",
        "    generated_text = blip_processor.batch_decode(\n",
        "        generated_ids,\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    generated_text = generated_text[0].strip().split(\"Question\")[0]\n",
        "\n",
        "    # Update memory\n",
        "    memory.append((question, generated_text))\n",
        "\n",
        "    # Assign to output\n",
        "    output.append_display_data(HTML(\"<b>USER:</b> \" + question))\n",
        "    output.append_display_data(HTML(\"<b>BLIP-2:</b> \" + generated_text))\n",
        "    output.append_display_data(HTML(\"<br>\"))\n",
        "\n",
        "# Prepare widgets\n",
        "in_text = widgets.Text()\n",
        "in_text.continuous_update = False\n",
        "in_text.observe(text_eventhandler, \"value\")\n",
        "output = widgets.Output()\n",
        "memory = []\n",
        "\n",
        "# Display chat box\n",
        "display(\n",
        "    widgets.VBox(\n",
        "        children=[output, in_text],\n",
        "        layout=widgets.Layout(display=\"inline-flex\", flex_flow=\"column-reverse\"),\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOHMAa6_MEgX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}