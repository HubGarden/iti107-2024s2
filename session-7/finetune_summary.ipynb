{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28304084-e6f6-45cf-9076-8dc439ef13e4",
   "metadata": {},
   "source": [
    "# Fine-Tune a Causal Language Model for Dialogue Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc73e16b-474b-40e9-bbaf-6f43479f018b",
   "metadata": {},
   "source": [
    "In this exercise, you will fine-tune a small Llama-like LLM called TinyLlama, for enhanced dialogue summarization. We will explore how to use the Huggingface TRL (Transformer Reinforcement Learning) library to help us to perform Supervised Finetuning (SFT).  We will explore the use of Parameter Efficient Fine-Tuning (PEFT) for efficient and fast finetuning, and evaluate the resulting model using ROUGE metrics.\n",
    "\n",
    "The techniques you learnt will allow you to fine-tune other more powerful models such as LLama 3 for domain-specific application of LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f022ce84-192b-4f62-8385-2cf571289117",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -q accelerate peft bitsandbytes transformers trl sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b5d120-fd43-4d6f-a2e1-cd2fcb16a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c84ed7ea-3707-445a-a266-e4181546f2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12011343-2a10-426a-bf38-6fc3f43766f7",
   "metadata": {},
   "source": [
    "## Templating Instruction Data\n",
    "\n",
    "To have the LLM follow instructions, we will need to prepare instruction data that follows a chat template. \n",
    "\n",
    "<img src=\"./chat_template.png\" />\n",
    "\n",
    "This chat template differentiates between what the LLM has generated and what the user has generated. May LLM chat models that are available on HuggingFace comes with built-in chat template that you can use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16862cf7-29c7-47b0-8f33-053428b2bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the chat model of TinyLlama. We only load it because we want to use it's chat template to format our data\n",
    "chat_model=\"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "template_tokenizer = AutoTokenizer.from_pretrained(chat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fff006d-9431-4748-9f4f-b0ce081d7295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\\n    {%- else %}\\n        {%- set date_string = \"26 Jul 2024\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n        {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n        {{- \\'\"parameters\": \\' }}\\n        {{- tool_call.arguments | tojson }}\\n        {{- \"}\" }}\\n        {{- \"<|eot_id|>\" }}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_tokenizer.get_chat_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60b383b-aad1-4ea6-a576-62d27aa44167",
   "metadata": {},
   "source": [
    "You can see that the template expects the prompt to include fields like role, content, and with content demarcated by `|user|`, `|assistant|` and `|system|`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c592d791-896d-41d2-8db8-b1d73e49971e",
   "metadata": {},
   "source": [
    "### Format the data according to chat template \n",
    "\n",
    "Let's download our data and format them according to the template given. We select a subset of 6000 samples to reduce training time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5511e86-af57-4f67-8546-75783fb92253",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset_train = load_dataset(dataset_name, split='train[:3000]')\n",
    "dataset_val = load_dataset(dataset_name, split='validation')\n",
    "dataset_test = load_dataset(dataset_name, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa20ede9-b3b7-411e-ae5b-fdc82bcf9a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a6ebda-b3a7-4b57-a43d-7ec54552a04c",
   "metadata": {},
   "source": [
    "Note that the completed prompt is put under 'text' field of the json. This is the default field that model will look for the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c6928d0-c83b-41e8-a2c3-a63827b6c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_template(row):\n",
    "    user_prompt = (\n",
    "        f\"Summarize this dialog:\\n{{dialog}}\\n---\\nSummary:\\n\"\n",
    "    )\n",
    "    user_prompt = user_prompt.format(dialog = row[\"dialogue\"])\n",
    "    row_json = [ {\"role\": \"system\", \"content\": \"You are a helpful assistant\" },\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "               {\"role\": \"assistant\", \"content\": row[\"summary\"]}]\n",
    "\n",
    "    prompt = template_tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    # print(prompt)\n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "debee8c3-bec5-420a-9535-e6bbb7afe3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63eceaa78b924aab83ada6caf071a398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_train = dataset_train.map(format_chat_template, remove_columns=list(dataset_train.features))\n",
    "dataset_val = dataset_val.map(format_chat_template, remove_columns=list(dataset_val.features))\n",
    "dataset_test = dataset_test.map(format_chat_template, remove_columns=list(dataset_test.features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2265ec5-a1d7-429a-b4d5-4757377b2ced",
   "metadata": {},
   "source": [
    "Using the \"text\" column, we can explore these formatted prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed2558fe-dd49-436e-82ea-220564ea69d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 Oct 2024\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSummarize this dialog:\\n#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\\n---\\nSummary:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nMr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.<|eot_id|>\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc659d-5e7c-4114-a4ae-2c27ccf4a2c1",
   "metadata": {},
   "source": [
    "### Model Quantization\n",
    "\n",
    "Now that we have our data, we can start loading in our model. This is where we apply the Q in QLoRA, namely quantization. We use the\n",
    "bitsandbytes package to compress the pretrained model to a 4-bit representation.\n",
    "\n",
    "In BitsAndBytesConfig, you can define the quantization scheme. We follow the steps used in the original QLoRA paper and load the model in 4-bit (load_in_4bit) with a normalized float representation (bnb_4bit_quant_type) and double quantization (bnb_4bit_use_double_quant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac96ae31-af8a-40f7-a39e-b4649ba1b837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# 4-bit quantization configuration - Q in QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # Use 4-bit precision model loading\n",
    "    bnb_4bit_quant_type=\"nf4\", # Quantization type\n",
    "    bnb_4bit_compute_dtype=\"float16\", # Compute dtype\n",
    "    bnb_4bit_use_double_quant=True, # Apply nested quantization\n",
    ")\n",
    "\n",
    "# Load the model to train on the GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cuda:0\",\n",
    "    # Leave this out for regular SFT\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e19a36-b88b-4c9b-94f1-fc7c4775bac5",
   "metadata": {},
   "source": [
    "### Test the Model with Zero Shot Inferencing\n",
    "\n",
    "Let's test the base model (non-instruction tuned model) with zero shot inferencing (i.e. ask it to summarize without giving any example. You can see that the model struggles to summarize the dialogue compared to the baseline summary, and it is just repeating the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f9e56be-0cfc-46bd-bbb6-0b3a6e7cc821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize this dialog:\n",
      "#Person1#: I have a problem with my cable.\n",
      "#Person2#: What about it?\n",
      "#Person1#: My cable has been out for the past week or so.\n",
      "#Person2#: The cable is down right now. I am very sorry.\n",
      "#Person1#: When will it be working again?\n",
      "#Person2#: It should be back on in the next couple of days.\n",
      "#Person1#: Do I still have to pay for the cable?\n",
      "#Person2#: We're going to give you a credit while the cable is down.\n",
      "#Person1#: So, I don't have to pay for it?\n",
      "#Person2#: No, not until your cable comes back on.\n",
      "#Person1#: Okay, thanks for everything.\n",
      "#Person2#: You're welcome, and I apologize for the inconvenience.\n",
      "---\n",
      "Summary:\n",
      "#Person1# has a problem with his cable.\n",
      "#Person2# asks what's wrong.\n",
      "#Person1# says his cable is down.\n",
      "#Person2# says he's sorry.\n",
      "#Person1# asks when it will be working again.\n",
      "#Person2# says it should be back on in the next couple of days.\n",
      "#Person1# asks if he still has to pay for the cable.\n",
      "#Person2# says no, not until his cable comes back on.\n",
      "#Person1# says okay, thanks for everything.\n",
      "#Person2# says you're welcome, and I apologize for the inconvenience.\n",
      "#Person1# says I'm sorry, too.\n",
      "#Person2# says you are, and I'm sorry.\n",
      "#Person1# says okay, thanks, and thanks, too.\n",
      "#Person2# says you're welcome, and thanks, too.\n",
      "#Person1# says I'm sorry, too, thanks, and thanks, too.\n",
      "#Person2# says\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"\n",
    "Summarize this dialog:\n",
    "#Person1#: I have a problem with my cable.\n",
    "#Person2#: What about it?\n",
    "#Person1#: My cable has been out for the past week or so.\n",
    "#Person2#: The cable is down right now. I am very sorry.\n",
    "#Person1#: When will it be working again?\n",
    "#Person2#: It should be back on in the next couple of days.\n",
    "#Person1#: Do I still have to pay for the cable?\n",
    "#Person2#: We're going to give you a credit while the cable is down.\n",
    "#Person1#: So, I don't have to pay for it?\n",
    "#Person2#: No, not until your cable comes back on.\n",
    "#Person1#: Okay, thanks for everything.\n",
    "#Person2#: You're welcome, and I apologize for the inconvenience.\n",
    "---\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():   # no gradient update\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=200)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9cad99-f525-4de6-9458-23affaa28b47",
   "metadata": {},
   "source": [
    "### LoRA Configuration\n",
    "\n",
    "We will be using LoRA to train our model. LoRA is supported in Hugging Face's PEFT library. \n",
    "Here are some explanation about the parameters used in the LoRA: \n",
    "- `r` - This is the rank of the compressed matrices. Increasing this value will also increase the sizes of compressed matrices leading to less compression and thereby improved representative power. Values typically range between 4 and 64.\n",
    "- `lora_alpha` - Controls the amount of change that is added to the original weights. In essence, it balances the knowledge of the original model with that of the new task. A rule of thumb is to choose a value twice the size of r.\n",
    "- `target_modules` - Controls which layers to target. The LoRA procedure can choose to ignore specific layers, like specific projection layers. This can speed up training but reduce performance and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b180947c-ddc3-4bda-980f-5afd27199d21",
   "metadata": {
    "id": "0tYs1ZhYDyw9"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# Prepare LoRA Configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,  # LoRA Scaling\n",
    "    lora_dropout=0.1,  # Dropout for LoRA Layers\n",
    "    r=64,  # Rank\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=  # Layers to target\n",
    "     ['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
    ")\n",
    "\n",
    "# prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f12c6f-df14-4b1d-8e1d-a15c22bd029e",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "\n",
    "Next we need to set our training configuration. Since we are going to use SFTTrainer, we can specify the training arguments in SFTConfig. \n",
    "\n",
    "Note that we set `fp16` to True for mixed-precision training. If you are using Ampere and newer GPU architecture, you can set bf16 to better accuracy and faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de5f9d4e-3472-4898-9be2-f9f40a27fe32",
   "metadata": {
    "id": "TwxZkx80G6bO"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Configure the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# where to write the checkpoint to\n",
    "output_dir = \"./results\"\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=5,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    # num_train_epochs=1,\n",
    "    logging_steps=5,\n",
    "    max_steps=30,\n",
    "    bf16=True,\n",
    "    # fp16=True\n",
    "    gradient_checkpointing=True,\n",
    "    resume_from_checkpoint=True,\n",
    "    packing=True, \n",
    "    eval_packing=True,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps=5,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=5,\n",
    "    run_name=\"llama3.2-summarize\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9290aac1-0c8e-4d90-ba70-1b03757c9d32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "1b741e85fcf1458f9875c12a9640dfee",
      "d034f23fd4df4e3296477d8dd76be5b1",
      "bee0b0ce2fb84c5eb67a04ced69752d1",
      "4d39d09e1e2648b1b5295f192e9ad356",
      "3f733eb54fe54d879c97dba7a5204ddd",
      "e9dab506c3b242d7b6228394ada6084b",
      "e7d4893b696c4941bf29d349eb2ceabb",
      "6d7ee17aa7024c8088c374781348f9f0",
      "7d8478e66e394f0fb077853e5319ee6a",
      "630e317f036f41f4a9852f7df81eef83",
      "eb35394940c74f60abe2daaeb243fa88"
     ]
    },
    "executionInfo": {
     "elapsed": 774977,
     "status": "ok",
     "timestamp": 1719391399990,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "B2D7RVihsE7Z",
    "outputId": "1a9f8125-6d39-410e-ff94-9a9ac493ff25"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/home/ubuntu/miniconda3/envs/llmenv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:421: UserWarning: You passed `packing=True` to the SFTTrainer/SFTConfig, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/llmenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/30 03:52 < 03:52, 0.06 it/s, Epoch 1.18/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.298400</td>\n",
       "      <td>2.098717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.010700</td>\n",
       "      <td>1.904796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.874900</td>\n",
       "      <td>1.831629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/llmenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/ubuntu/miniconda3/envs/llmenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/ubuntu/miniconda3/envs/llmenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 16\u001b[0m\n\u001b[1;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      6\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset_train,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     args\u001b[38;5;241m=\u001b[39msft_config\n\u001b[1;32m     13\u001b[0m  )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:434\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 434\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.10/site-packages/transformers/trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.10/site-packages/transformers/trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3516\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3518\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.10/site-packages/accelerate/accelerator.py:2246\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2246\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,\n",
    "    # dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    # Leave this out for regular SFT\n",
    "    peft_config=peft_config,\n",
    "    args=sft_config\n",
    " )\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36cac77f-d8b5-4c66-bfcf-5c95c0037ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save QLoRA weights\n",
    "trainer.model.save_pretrained(\"Llama-3.2-1B-Summarizer-QLoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fcace21-7c4a-4b4a-9e5c-7383de2dd1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max context lenth: 131072 in max_position_embeddings\n",
      "Maximum Context length:  131072\n"
     ]
    }
   ],
   "source": [
    "def get_max_context_length(model):\n",
    "    \n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    \n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max context lenth: {max_length} in {length_setting}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max context length: {max_length}\")\n",
    "        \n",
    "    return max_length\n",
    "\n",
    "max_context_length = get_max_context_length(model)\n",
    "print('Maximum Context length: ', max_context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe985335-acfa-4ea2-95de-05fd4088286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"Llama-3.2-1B-Summarizer-QLoRA\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7db04c34-623f-4bec-8a85-b5e3024862ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|user|>\n",
      "Summarize this dialog:\n",
      "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
      "B: I’m pretty sure I am. What’s up?\n",
      "A: Can you go with me to the animal shelter?.\n",
      "B: What do you want to do?\n",
      "A: I want to get a puppy for my son.\n",
      "B: That will make him so happy.\n",
      "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
      "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)\n",
      "A: I'll get him one of those little dogs.\n",
      "B: One that won't grow up too big;-)\n",
      "A: And eat too much;-))\n",
      "B: Do you know which one he would like?\n",
      "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
      "B: I bet you had to drag him away.\n",
      "A: He wanted to take it home right away ;-).\n",
      "B: I wonder what he'll name it.\n",
      "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
      "---\n",
      "Summary:</s>\n",
      "<|assistant|>\n",
      "#user# wants to get a puppy for #user#'s son. #user# suggests getting a little dog that won't grow up too big. #user# also thinks #user#'s son will like the dog. #user# wonders what #user#'s son will name the dog. #user# thinks it will be a great Motorhead fan. #user# tells #user# about #user#'s son's hamster. #user# wants to get the dog for #user#'s son. #user# wants to get the dog from the animal shelter. #user# wants to know what #user#'s son will name the dog. #user# wants to get the dog for #user#'s son. #user# wants to know what #user#'s son will name the dog. #user# wants to get the dog for #user#'s son. #user# wants to know what #user#'s son will name the dog. #user# wants to get the dog for #user#'s son. #user# wants to know what #user#'s son will name the dog. #user# wants to get the dog for #user#'s son. #user# wants to know what #user#'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[128000,     27,     91,    882,     91,    397,   9370,   5730,    553,\n",
       "            420,   7402,    512,     32,     25,  21694,   8529,     11,    527,\n",
       "            499,  13326,  16986,    753,  13658,   5380,     33,     25,    358,\n",
       "           4344,   5128,   2771,    358,   1097,     13,   3639,    753,    709,\n",
       "           5380,     32,     25,   3053,    499,    733,    449,    757,    311,\n",
       "            279,  10065,  23756,     30,    627,     33,     25,   3639,    656,\n",
       "            499,   1390,    311,    656,   5380,     32,     25,    358,   1390,\n",
       "            311,    636,    264,  42289,    369,    856,   4538,    627,     33,\n",
       "             25,   3011,    690,   1304,   1461,    779,   6380,    627,     32,\n",
       "             25,  22335,     11,    584,   4070,  14407,    433,   1690,   3115,\n",
       "             13,    358,   1781,    568,    753,   5644,   1457,    627,     33,\n",
       "             25,   3011,    753,   1695,     13,    432,  51226,    264,   5679,\n",
       "            374,    264,  11292,   4360,     13,   9086,   3515,    264,   8945,\n",
       "          90608,    340,     32,     25,    358,   3358,    636,   1461,    832,\n",
       "            315,   1884,   2697,  12875,    627,     33,     25,   3861,    430,\n",
       "           2834,    956,   3139,    709,   2288,   2466,  55533,    340,     32,\n",
       "             25,   1628,   8343,   2288,   1790,  55533,   1192,     33,     25,\n",
       "           3234,    499,   1440,    902,    832,    568,   1053,   1093,   5380,\n",
       "             32,     25,   8840,     11,  10035,     11,    358,   3952,   1461,\n",
       "           1070,   1566,   7159,     13,   1283,   8710,    757,    832,    430,\n",
       "            568,   2216,  15262,    627,     33,     25,    358,   1297,    499,\n",
       "           1047,    311,  11161,   1461,   3201,    627,     32,     25,   1283,\n",
       "           4934,    311,   1935,    433,   2162,   1314,   3201,  90608,   4390,\n",
       "             33,     25,    358,   5895,   1148,    568,   3358,    836,    433,\n",
       "            627,     32,     25,   1283,   1071,    568,   7070,    836,    433,\n",
       "           1306,    813,   5710,  13824,   3751,   1389,  48506,   2465,    220,\n",
       "            482,    568,    596,    220,    264,   2294,  18079,   2025,   8571,\n",
       "          21629,   6054,  11192,  19791,   7920,     82,    397,     27,     91,\n",
       "          78191,     91,    397,      2,    882,      2,   6944,    311,    636,\n",
       "            264,  42289,    369,    674,    882,  95282,     82,   4538,     13,\n",
       "            674,    882,      2,  13533,   3794,    264,   2697,   5679,    430,\n",
       "           2834,    956,   3139,    709,   2288,   2466,     13,    674,    882,\n",
       "              2,   1101,  15849,    674,    882,  95282,     82,   4538,    690,\n",
       "           1093,    279,   5679,     13,    674,    882,      2,  40164,   1148,\n",
       "            674,    882,  95282,     82,   4538,    690,    836,    279,   5679,\n",
       "             13,    674,    882,      2,  15849,    433,    690,    387,    264,\n",
       "           2294,  18079,   2025,   8571,     13,    674,    882,      2,  10975,\n",
       "            674,    882,      2,    922,    674,    882,  95282,     82,   4538,\n",
       "            596,  13824,   3751,     13,    674,    882,      2,   6944,    311,\n",
       "            636,    279,   5679,    369,    674,    882,  95282,     82,   4538,\n",
       "             13,    674,    882,      2,   6944,    311,    636,    279,   5679,\n",
       "            505,    279,  10065,  23756,     13,    674,    882,      2,   6944,\n",
       "            311,   1440,   1148,    674,    882,  95282,     82,   4538,    690,\n",
       "            836,    279,   5679,     13,    674,    882,      2,   6944,    311,\n",
       "            636,    279,   5679,    369,    674,    882,  95282,     82,   4538,\n",
       "             13,    674,    882,      2,   6944,    311,   1440,   1148,    674,\n",
       "            882,  95282,     82,   4538,    690,    836,    279,   5679,     13,\n",
       "            674,    882,      2,   6944,    311,    636,    279,   5679,    369,\n",
       "            674,    882,  95282,     82,   4538,     13,    674,    882,      2,\n",
       "           6944,    311,   1440,   1148,    674,    882,  95282,     82,   4538,\n",
       "            690,    836,    279,   5679,     13,    674,    882,      2,   6944,\n",
       "            311,    636,    279,   5679,    369,    674,    882,  95282,     82,\n",
       "           4538,     13,    674,    882,      2,   6944,    311,   1440,   1148,\n",
       "            674,    882,  95282,     82,   4538,    690,    836,    279,   5679,\n",
       "             13,    674,    882,      2,   6944,    311,    636,    279,   5679,\n",
       "            369,    674,    882,  95282,     82,   4538,     13,    674,    882,\n",
       "              2,   6944,    311,   1440,   1148,    674,    882,  95282]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_prompt = \"\"\"<|user|>\n",
    "Summarize this dialog:\n",
    "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
    "B: I’m pretty sure I am. What’s up?\n",
    "A: Can you go with me to the animal shelter?.\n",
    "B: What do you want to do?\n",
    "A: I want to get a puppy for my son.\n",
    "B: That will make him so happy.\n",
    "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
    "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)\n",
    "A: I'll get him one of those little dogs.\n",
    "B: One that won't grow up too big;-)\n",
    "A: And eat too much;-))\n",
    "B: Do you know which one he would like?\n",
    "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
    "B: I bet you had to drag him away.\n",
    "A: He wanted to take it home right away ;-).\n",
    "B: I wonder what he'll name it.\n",
    "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
    "---\n",
    "Summary:</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "#Streaming support\n",
    "streamer = TextStreamer(tokenizer)\n",
    "merged_model.generate(**model_input, streamer=streamer, max_length=512)\n",
    "# merged_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     print(tokenizer.decode(merged_model.generate(**model_input)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e586894d-a59a-41fc-af0b-1baa67e734b0",
   "metadata": {},
   "source": [
    "Good reference:\n",
    "\n",
    "https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-tune-an-LLM-Part-3-The-HuggingFace-Trainer--Vmlldzo1OTEyNjMy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0003c98-555e-4943-a618-32e04be06b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4df3dd-3598-4993-aad6-5518a159a984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d7f98-9aa4-47c5-b943-eaf5e4a7f39d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
